{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b 8 times 4 radii\n",
      "\n",
      "(22000, 384)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_detectors = 6\n",
    "n_radii = 8\n",
    "n_angles = 8\n",
    "number_of_features = n_detectors*n_radii*n_angles\n",
    "input_data = []\n",
    "with open('data/PD033_time_norepeats_8radii_equidistant.dat', 'r') as inpf: \n",
    "    l = inpf.readline()\n",
    "    print(l)\n",
    "    for line in inpf:\n",
    "        features = []\n",
    "        s = line.strip().split()\n",
    "        b = float(s[0])\n",
    "        # for i in range(6):\n",
    "            # for digit in s[i + 1]:\n",
    "                # features.append(float(digit))\n",
    "        for digit in s[1:]:\n",
    "            features.append(float(digit))\n",
    "        input_data.append([b, features])\n",
    "    # print(len(input_data))\n",
    "    \n",
    "    np.random.shuffle(input_data)\n",
    "    input_sorted = sorted(input_data, key=lambda x: (x[0]))\n",
    "    # print(input_data)\n",
    "    \n",
    "    features = np.zeros((len(input_data), number_of_features))\n",
    "    labels = np.zeros((len(input_data), 1))\n",
    "    incr = 0\n",
    "    for elem in input_data:\n",
    "        labels[incr] = np.array(elem[0])\n",
    "        features[incr] = np.array(elem[1])\n",
    "        incr += 1\n",
    "    \n",
    "    # print(input_sorted[:5])\n",
    "    # print(input_data[:5])\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15.294]\n",
      " [11.586]\n",
      " [12.508]\n",
      " [ 9.306]\n",
      " [ 2.199]\n",
      " [ 5.458]\n",
      " [10.821]\n",
      " [10.183]\n",
      " [11.541]\n",
      " [10.183]]\n",
      "Length of segment: 5500\n",
      "Encoded into intervals: [(0, 6.974), (6.974, 9.814), (9.814, 12.043), (12.043, 16.347)]\n",
      "[[3]\n",
      " [2]\n",
      " [3]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]]\n",
      "Set shape: (22000, 384)\n",
      "Train features shape: torch.Size([17600, 384])\n",
      "Train labels shape: torch.Size([17600])\n",
      "Test features shape: torch.Size([4400, 384])\n",
      "Test labels shape: torch.Size([4400])\n"
     ]
    }
   ],
   "source": [
    "# Encode labels into segments with equal length\n",
    "def encode_equi_segment(b: float, maximum: float, groups: int) -> int:\n",
    "    if b == maximum:\n",
    "        return groups - 1\n",
    "    else:\n",
    "        length_of_segment = maximum / groups\n",
    "        return int( b // length_of_segment )\n",
    "\n",
    "    \n",
    "# Encode labels into segments with equal size\n",
    "def encode_equi_size(labels_arr: np.ndarray, groups: int ) -> np.ndarray:\n",
    "    lb_sorted = np.sort(labels_arr, axis=0)\n",
    "    segment = lb_sorted.shape[0]//groups\n",
    "    print(\"Length of segment: {}\".format(segment))\n",
    "    borders = []\n",
    "    \n",
    "    # define borders of segments\n",
    "    for i in range(groups):   \n",
    "        if i != (groups - 1) and i != 0:\n",
    "            borders.append((lb_sorted[(i)*segment].item(), lb_sorted[(i+1)*segment].item()))\n",
    "        elif i == 0:\n",
    "            borders.append((0, lb_sorted[(i+1)*segment].item()))\n",
    "        else: \n",
    "            borders.append((lb_sorted[(i)*segment].item(), lb_sorted[labels.shape[0] - 1].item()))\n",
    "    print(\"Encoded into intervals: {}\" .format(borders))\n",
    "    lb_enc = np.zeros_like(labels, dtype=np.int)\n",
    "    incr = 0\n",
    "    \n",
    "    # iterate over all (not sorted) labels and encode them\n",
    "    for elem in labels:   \n",
    "        gr = 0\n",
    "        for seg in borders:\n",
    "            if seg[0] < elem <= seg[1]:\n",
    "                lb_enc[incr] = gr\n",
    "                break\n",
    "            gr += 1\n",
    "        incr += 1\n",
    "    return lb_enc\n",
    "\n",
    "\n",
    "number_of_groups = 4\n",
    "\n",
    "\n",
    "# labels_encoded_list = []\n",
    "# maximum_b = np.max(labels)\n",
    "# for label in labels:\n",
    "#     labels_encoded_list.append(encode_equi_segment(label[0], maximum_b, number_of_groups))\n",
    "# labels_encoded = np.array(labels_encoded_list)\n",
    "# train_labels = torch.flatten(torch.tensor(labels_encoded[:size_of_training_set]))\n",
    "# test_labels = torch.flatten(torch.tensor(labels_encoded[size_of_training_set:]))\n",
    "\n",
    "\n",
    "# Divide into test and training sets\n",
    "size_of_training_set = int(features.shape[0] * 0.8)\n",
    "size_of_test_set = features.shape[0] - size_of_training_set\n",
    "\n",
    "train_features = torch.tensor(features[:size_of_training_set], dtype=torch.float32)\n",
    "test_features = torch.tensor(features[size_of_training_set:], dtype=torch.float32)\n",
    "\n",
    "print(labels[:10])\n",
    "labels_encoded_equisized = encode_equi_size(labels, number_of_groups)\n",
    "print(labels_encoded_equisized[:10])\n",
    "train_labels = torch.flatten(torch.tensor(labels_encoded_equisized[:size_of_training_set]))\n",
    "test_labels = torch.flatten(torch.tensor(labels_encoded_equisized[size_of_training_set:]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Set shape: ' + str(features.shape))\n",
    "print('Train features shape: ' + str(train_features.shape))\n",
    "print('Train labels shape: ' + str(train_labels.shape))\n",
    "print('Test features shape: ' + str(test_features.shape))\n",
    "print('Test labels shape: ' + str(test_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Fully connected network. \n",
    "# Convolutional - three blocks below\n",
    "\n",
    "def get_correct_predictions(preds: torch.Tensor, values: torch.Tensor) -> int:\n",
    "    return preds.argmax(dim=1).eq(values).sum().item()\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.lin1 = nn.Linear(in_features = number_of_features, out_features = 64)\n",
    "        nn.init.normal_(self.lin1.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "        self.lin2 = nn.Linear(in_features = 64, out_features = 16)\n",
    "        nn.init.normal_(self.lin2.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "        # self.lin3 = nn.Linear(in_features = 16, out_features = 30)\n",
    "        # nn.init.normal_(self.lin3.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "        # self.lin4 = nn.Linear(in_features = 30, out_features = 20)\n",
    "        # nn.init.normal_(self.lin4.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "        # self.lin5 = nn.Linear(in_features = 20, out_features = 12)\n",
    "        # nn.init.normal_(self.lin5.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "        # self.lin6 = nn.Linear(in_features = 12, out_features = 8)\n",
    "        # nn.init.normal_(self.lin6.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "        self.out = nn.Linear(in_features = 16, out_features = number_of_groups)\n",
    "        nn.init.normal_(self.out.weight, mean=0.0, std=0.02)\n",
    "        # print(self.lin1.weight[:4, :8])\n",
    "        # print(self.lin2.weight[:4, :8])\n",
    "        # print(self.lin3.weight[:4, :8])\n",
    "        # print(self.out.weight)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        t = t\n",
    "        t = F.relu(self.lin1(t))\n",
    "        t = F.relu(self.lin2(t))\n",
    "        # t = F.relu(self.lin3(t))\n",
    "        # t = F.relu(self.lin4(t))\n",
    "        # t = F.relu(self.lin5(t))\n",
    "        # t = F.relu(self.lin6(t))\n",
    "        t = F.softmax(self.out(t), dim=1)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0284,  0.0065, -0.0025,  0.0198,  0.0251, -0.0125, -0.0025, -0.0003,\n",
      "         -0.0133, -0.0237, -0.0240, -0.0231,  0.0025,  0.0218,  0.0169, -0.0080],\n",
      "        [ 0.0276,  0.0265,  0.0238, -0.0310, -0.0089, -0.0036,  0.0020, -0.0073,\n",
      "          0.0071,  0.0102, -0.0115, -0.0141,  0.0354, -0.0143, -0.0236, -0.0216],\n",
      "        [-0.0233,  0.0166,  0.0464, -0.0086, -0.0254,  0.0245, -0.0237,  0.0234,\n",
      "         -0.0167, -0.0046, -0.0293, -0.0051, -0.0023,  0.0006,  0.0323, -0.0216],\n",
      "        [-0.0558,  0.0041,  0.0056,  0.0219,  0.0010, -0.0105,  0.0165, -0.0079,\n",
      "          0.0012,  0.0025,  0.0392,  0.0017, -0.0137,  0.0122, -0.0212, -0.0314]],\n",
      "       requires_grad=True)\n",
      "Epoch:    0 |---> loss is 1.3863229752, total correct predictions:  4362, its 24.784%\n",
      "Epoch:    1 |---> loss is 1.3831715584, total correct predictions:  5679, its 32.267%\n",
      "Epoch:    2 |---> loss is 1.3682519197, total correct predictions:  4443, its 25.244%\n",
      "Epoch:    3 |---> loss is 1.3431073427, total correct predictions:  5685, its 32.301%\n",
      "Epoch:    4 |---> loss is 1.3313522339, total correct predictions:  5685, its 32.301%\n",
      "Epoch:    5 |---> loss is 1.3406087160, total correct predictions:  5685, its 32.301%\n",
      "Epoch:    6 |---> loss is 1.3403613567, total correct predictions:  5726, its 32.534%\n",
      "Epoch:    7 |---> loss is 1.3301688433, total correct predictions:  6061, its 34.438%\n",
      "Epoch:    8 |---> loss is 1.3255325556, total correct predictions:  6200, its 35.227%\n",
      "Epoch:    9 |---> loss is 1.3254398108, total correct predictions:  6546, its 37.193%\n",
      "Epoch:   10 |---> loss is 1.3241841793, total correct predictions:  6827, its 38.790%\n",
      "Epoch:   11 |---> loss is 1.3184131384, total correct predictions:  6829, its 38.801%\n",
      "Epoch:   12 |---> loss is 1.3148680925, total correct predictions:  6767, its 38.449%\n",
      "Epoch:   13 |---> loss is 1.3143430948, total correct predictions:  6773, its 38.483%\n",
      "Epoch:   14 |---> loss is 1.3124563694, total correct predictions:  6916, its 39.295%\n",
      "Epoch:   15 |---> loss is 1.3084068298, total correct predictions:  7244, its 41.159%\n",
      "Epoch:   16 |---> loss is 1.3052235842, total correct predictions:  7615, its 43.267%\n",
      "Epoch:   17 |---> loss is 1.3037534952, total correct predictions:  7824, its 44.455%\n",
      "Epoch:   18 |---> loss is 1.3004391193, total correct predictions:  7870, its 44.716%\n",
      "Epoch:   19 |---> loss is 1.2962892056, total correct predictions:  7850, its 44.602%\n",
      "Epoch:   20 |---> loss is 1.2927383184, total correct predictions:  7815, its 44.403%\n",
      "Epoch:   21 |---> loss is 1.2888983488, total correct predictions:  7815, its 44.403%\n",
      "Epoch:   22 |---> loss is 1.2838164568, total correct predictions:  7868, its 44.705%\n",
      "Epoch:   23 |---> loss is 1.2789535522, total correct predictions:  8069, its 45.847%\n",
      "Epoch:   24 |---> loss is 1.2734907866, total correct predictions:  8250, its 46.875%\n",
      "Epoch:   25 |---> loss is 1.2656100988, total correct predictions:  8543, its 48.540%\n",
      "Epoch:   26 |---> loss is 1.2578642368, total correct predictions:  8860, its 50.341%\n",
      "Epoch:   27 |---> loss is 1.2491614819, total correct predictions:  9230, its 52.443%\n",
      "Epoch:   28 |---> loss is 1.2397298813, total correct predictions:  9432, its 53.591%\n",
      "Epoch:   29 |---> loss is 1.2301591635, total correct predictions:  9551, its 54.267%\n",
      "Epoch:   30 |---> loss is 1.2188472748, total correct predictions:  9686, its 55.034%\n",
      "Epoch:   31 |---> loss is 1.2083976269, total correct predictions:  9828, its 55.841%\n",
      "Epoch:   32 |---> loss is 1.1970790625, total correct predictions: 10045, its 57.074%\n",
      "Epoch:   33 |---> loss is 1.1861044168, total correct predictions: 10305, its 58.551%\n",
      "Epoch:   34 |---> loss is 1.1742460728, total correct predictions: 10523, its 59.790%\n",
      "Epoch:   35 |---> loss is 1.1622581482, total correct predictions: 10774, its 61.216%\n",
      "Epoch:   36 |---> loss is 1.1513073444, total correct predictions: 11019, its 62.608%\n",
      "Epoch:   37 |---> loss is 1.1399352551, total correct predictions: 11124, its 63.205%\n",
      "Epoch:   38 |---> loss is 1.1285485029, total correct predictions: 11303, its 64.222%\n",
      "Epoch:   39 |---> loss is 1.1187726259, total correct predictions: 11404, its 64.795%\n",
      "Epoch:   40 |---> loss is 1.1104819775, total correct predictions: 11474, its 65.193%\n",
      "Epoch:   41 |---> loss is 1.1009500027, total correct predictions: 11594, its 65.875%\n",
      "Epoch:   42 |---> loss is 1.0920498371, total correct predictions: 11745, its 66.733%\n",
      "Epoch:   43 |---> loss is 1.0850332975, total correct predictions: 11837, its 67.256%\n",
      "Epoch:   44 |---> loss is 1.0794333220, total correct predictions: 11899, its 67.608%\n",
      "Epoch:   45 |---> loss is 1.0747951269, total correct predictions: 11978, its 68.057%\n",
      "Epoch:   46 |---> loss is 1.0677462816, total correct predictions: 12034, its 68.375%\n",
      "Epoch:   47 |---> loss is 1.0613526106, total correct predictions: 12190, its 69.261%\n",
      "Epoch:   48 |---> loss is 1.0565369129, total correct predictions: 12239, its 69.540%\n",
      "Epoch:   49 |---> loss is 1.0525773764, total correct predictions: 12316, its 69.977%\n",
      "Epoch:   50 |---> loss is 1.0486702919, total correct predictions: 12389, its 70.392%\n",
      "Epoch:   51 |---> loss is 1.0435618162, total correct predictions: 12476, its 70.886%\n",
      "Epoch:   52 |---> loss is 1.0399799347, total correct predictions: 12546, its 71.284%\n",
      "Epoch:   53 |---> loss is 1.0422183275, total correct predictions: 12416, its 70.545%\n",
      "Epoch:   54 |---> loss is 1.0430463552, total correct predictions: 12415, its 70.540%\n",
      "Epoch:   55 |---> loss is 1.0335948467, total correct predictions: 12630, its 71.761%\n",
      "Epoch:   56 |---> loss is 1.0393894911, total correct predictions: 12486, its 70.943%\n",
      "Epoch:   57 |---> loss is 1.0261100531, total correct predictions: 12796, its 72.705%\n",
      "Epoch:   58 |---> loss is 1.0347651243, total correct predictions: 12550, its 71.307%\n",
      "Epoch:   59 |---> loss is 1.0216059685, total correct predictions: 12864, its 73.091%\n",
      "Epoch:   60 |---> loss is 1.0257925987, total correct predictions: 12744, its 72.409%\n",
      "Epoch:   61 |---> loss is 1.0161981583, total correct predictions: 12938, its 73.511%\n",
      "Epoch:   62 |---> loss is 1.0196231604, total correct predictions: 12829, its 72.892%\n",
      "Epoch:   63 |---> loss is 1.0134797096, total correct predictions: 12965, its 73.665%\n",
      "Epoch:   64 |---> loss is 1.0122301579, total correct predictions: 13000, its 73.864%\n",
      "Epoch:   65 |---> loss is 1.0112290382, total correct predictions: 13048, its 74.136%\n",
      "Epoch:   66 |---> loss is 1.0053831339, total correct predictions: 13167, its 74.812%\n",
      "Epoch:   67 |---> loss is 1.0088130236, total correct predictions: 13073, its 74.278%\n",
      "Epoch:   68 |---> loss is 1.0025516748, total correct predictions: 13230, its 75.170%\n",
      "Epoch:   69 |---> loss is 1.0039507151, total correct predictions: 13180, its 74.886%\n",
      "Epoch:   70 |---> loss is 0.9977701306, total correct predictions: 13317, its 75.665%\n",
      "Epoch:   71 |---> loss is 0.9998521805, total correct predictions: 13254, its 75.307%\n",
      "Epoch:   72 |---> loss is 0.9961252809, total correct predictions: 13347, its 75.835%\n",
      "Epoch:   73 |---> loss is 0.9943861961, total correct predictions: 13396, its 76.114%\n",
      "Epoch:   74 |---> loss is 0.9930061698, total correct predictions: 13430, its 76.307%\n",
      "Epoch:   75 |---> loss is 0.9907549620, total correct predictions: 13450, its 76.420%\n",
      "Epoch:   76 |---> loss is 0.9909724593, total correct predictions: 13407, its 76.176%\n",
      "Epoch:   77 |---> loss is 0.9873111248, total correct predictions: 13525, its 76.847%\n",
      "Epoch:   78 |---> loss is 0.9869888425, total correct predictions: 13538, its 76.920%\n",
      "Epoch:   79 |---> loss is 0.9834870100, total correct predictions: 13582, its 77.170%\n",
      "Epoch:   80 |---> loss is 0.9834288359, total correct predictions: 13584, its 77.182%\n",
      "Epoch:   81 |---> loss is 0.9807341695, total correct predictions: 13631, its 77.449%\n",
      "Epoch:   82 |---> loss is 0.9813815951, total correct predictions: 13624, its 77.409%\n",
      "Epoch:   83 |---> loss is 0.9815433025, total correct predictions: 13630, its 77.443%\n",
      "Epoch:   84 |---> loss is 0.9872151613, total correct predictions: 13516, its 76.795%\n",
      "Epoch:   85 |---> loss is 0.9947214723, total correct predictions: 13302, its 75.580%\n",
      "Epoch:   86 |---> loss is 0.9785143137, total correct predictions: 13648, its 77.545%\n",
      "Epoch:   87 |---> loss is 0.9760246277, total correct predictions: 13706, its 77.875%\n",
      "Epoch:   88 |---> loss is 0.9831630588, total correct predictions: 13536, its 76.909%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   89 |---> loss is 0.9724601507, total correct predictions: 13755, its 78.153%\n",
      "Epoch:   90 |---> loss is 0.9773349762, total correct predictions: 13670, its 77.670%\n",
      "Epoch:   91 |---> loss is 0.9783627987, total correct predictions: 13643, its 77.517%\n",
      "Epoch:   92 |---> loss is 0.9700539112, total correct predictions: 13772, its 78.250%\n",
      "Epoch:   93 |---> loss is 0.9785763025, total correct predictions: 13668, its 77.659%\n",
      "Epoch:   94 |---> loss is 0.9721490145, total correct predictions: 13763, its 78.199%\n",
      "Epoch:   95 |---> loss is 0.9701007605, total correct predictions: 13794, its 78.375%\n",
      "Epoch:   96 |---> loss is 0.9727788568, total correct predictions: 13751, its 78.131%\n",
      "Epoch:   97 |---> loss is 0.9653392434, total correct predictions: 13861, its 78.756%\n",
      "Epoch:   98 |---> loss is 0.9694759846, total correct predictions: 13789, its 78.347%\n",
      "Epoch:   99 |---> loss is 0.9640378356, total correct predictions: 13874, its 78.830%\n",
      "Epoch:  100 |---> loss is 0.9661514759, total correct predictions: 13838, its 78.625%\n",
      "Epoch:  101 |---> loss is 0.9638764262, total correct predictions: 13883, its 78.881%\n",
      "Epoch:  102 |---> loss is 0.9625917673, total correct predictions: 13899, its 78.972%\n",
      "Epoch:  103 |---> loss is 0.9632882476, total correct predictions: 13900, its 78.977%\n",
      "Epoch:  104 |---> loss is 0.9597440958, total correct predictions: 13937, its 79.188%\n",
      "Epoch:  105 |---> loss is 0.9616522789, total correct predictions: 13933, its 79.165%\n",
      "Epoch:  106 |---> loss is 0.9584264159, total correct predictions: 13960, its 79.318%\n",
      "Epoch:  107 |---> loss is 0.9592199922, total correct predictions: 13962, its 79.330%\n",
      "Epoch:  108 |---> loss is 0.9581659436, total correct predictions: 13969, its 79.369%\n",
      "Epoch:  109 |---> loss is 0.9566226602, total correct predictions: 13988, its 79.477%\n",
      "Epoch:  110 |---> loss is 0.9573838115, total correct predictions: 13986, its 79.466%\n",
      "Epoch:  111 |---> loss is 0.9550623894, total correct predictions: 14006, its 79.580%\n",
      "Epoch:  112 |---> loss is 0.9554159045, total correct predictions: 14005, its 79.574%\n",
      "Epoch:  113 |---> loss is 0.9545251727, total correct predictions: 14026, its 79.693%\n",
      "Epoch:  114 |---> loss is 0.9533033371, total correct predictions: 14032, its 79.727%\n",
      "Epoch:  115 |---> loss is 0.9535964131, total correct predictions: 14034, its 79.739%\n",
      "Epoch:  116 |---> loss is 0.9521923661, total correct predictions: 14049, its 79.824%\n",
      "Epoch:  117 |---> loss is 0.9518936276, total correct predictions: 14057, its 79.869%\n",
      "Epoch:  118 |---> loss is 0.9516338706, total correct predictions: 14062, its 79.898%\n",
      "Epoch:  119 |---> loss is 0.9504752159, total correct predictions: 14070, its 79.943%\n",
      "Epoch:  120 |---> loss is 0.9505484104, total correct predictions: 14075, its 79.972%\n",
      "Epoch:  121 |---> loss is 0.9498069882, total correct predictions: 14081, its 80.006%\n",
      "Epoch:  122 |---> loss is 0.9491130114, total correct predictions: 14087, its 80.040%\n",
      "Epoch:  123 |---> loss is 0.9491046667, total correct predictions: 14086, its 80.034%\n",
      "Epoch:  124 |---> loss is 0.9482020140, total correct predictions: 14093, its 80.074%\n",
      "Epoch:  125 |---> loss is 0.9478884339, total correct predictions: 14099, its 80.108%\n",
      "Epoch:  126 |---> loss is 0.9476306438, total correct predictions: 14105, its 80.142%\n",
      "Epoch:  127 |---> loss is 0.9468687773, total correct predictions: 14109, its 80.165%\n",
      "Epoch:  128 |---> loss is 0.9467874169, total correct predictions: 14109, its 80.165%\n",
      "Epoch:  129 |---> loss is 0.9462628961, total correct predictions: 14115, its 80.199%\n",
      "Epoch:  130 |---> loss is 0.9458658099, total correct predictions: 14118, its 80.216%\n",
      "Epoch:  131 |---> loss is 0.9456803799, total correct predictions: 14120, its 80.227%\n",
      "Epoch:  132 |---> loss is 0.9451456070, total correct predictions: 14124, its 80.250%\n",
      "Epoch:  133 |---> loss is 0.9449180961, total correct predictions: 14127, its 80.267%\n",
      "Epoch:  134 |---> loss is 0.9445635676, total correct predictions: 14131, its 80.290%\n",
      "Epoch:  135 |---> loss is 0.9441268444, total correct predictions: 14139, its 80.335%\n",
      "Epoch:  136 |---> loss is 0.9438831806, total correct predictions: 14142, its 80.352%\n",
      "Epoch:  137 |---> loss is 0.9435071945, total correct predictions: 14146, its 80.375%\n",
      "Epoch:  138 |---> loss is 0.9431434274, total correct predictions: 14150, its 80.398%\n",
      "Epoch:  139 |---> loss is 0.9429061413, total correct predictions: 14155, its 80.426%\n",
      "Epoch:  140 |---> loss is 0.9425500631, total correct predictions: 14158, its 80.443%\n",
      "Epoch:  141 |---> loss is 0.9422518611, total correct predictions: 14161, its 80.460%\n",
      "Epoch:  142 |---> loss is 0.9420146942, total correct predictions: 14166, its 80.489%\n",
      "Epoch:  143 |---> loss is 0.9416493773, total correct predictions: 14167, its 80.494%\n",
      "Epoch:  144 |---> loss is 0.9413713813, total correct predictions: 14172, its 80.523%\n",
      "Epoch:  145 |---> loss is 0.9410870671, total correct predictions: 14174, its 80.534%\n",
      "Epoch:  146 |---> loss is 0.9407407641, total correct predictions: 14178, its 80.557%\n",
      "Epoch:  147 |---> loss is 0.9404599667, total correct predictions: 14183, its 80.585%\n",
      "Epoch:  148 |---> loss is 0.9401534796, total correct predictions: 14186, its 80.602%\n",
      "Epoch:  149 |---> loss is 0.9397838116, total correct predictions: 14193, its 80.642%\n",
      "Epoch:  150 |---> loss is 0.9394661188, total correct predictions: 14198, its 80.670%\n",
      "Epoch:  151 |---> loss is 0.9391705990, total correct predictions: 14206, its 80.716%\n",
      "Epoch:  152 |---> loss is 0.9388511777, total correct predictions: 14209, its 80.733%\n",
      "Epoch:  153 |---> loss is 0.9386041760, total correct predictions: 14212, its 80.750%\n",
      "Epoch:  154 |---> loss is 0.9383718967, total correct predictions: 14214, its 80.761%\n",
      "Epoch:  155 |---> loss is 0.9380941391, total correct predictions: 14221, its 80.801%\n",
      "Epoch:  156 |---> loss is 0.9378433228, total correct predictions: 14223, its 80.812%\n",
      "Epoch:  157 |---> loss is 0.9375957847, total correct predictions: 14226, its 80.830%\n",
      "Epoch:  158 |---> loss is 0.9373350739, total correct predictions: 14230, its 80.852%\n",
      "Epoch:  159 |---> loss is 0.9370678663, total correct predictions: 14234, its 80.875%\n",
      "Epoch:  160 |---> loss is 0.9368407130, total correct predictions: 14235, its 80.881%\n",
      "Epoch:  161 |---> loss is 0.9366037846, total correct predictions: 14237, its 80.892%\n",
      "Epoch:  162 |---> loss is 0.9363833070, total correct predictions: 14241, its 80.915%\n",
      "Epoch:  163 |---> loss is 0.9361670017, total correct predictions: 14246, its 80.943%\n",
      "Epoch:  164 |---> loss is 0.9359329343, total correct predictions: 14246, its 80.943%\n",
      "Epoch:  165 |---> loss is 0.9356953502, total correct predictions: 14249, its 80.960%\n",
      "Epoch:  166 |---> loss is 0.9354668856, total correct predictions: 14254, its 80.989%\n",
      "Epoch:  167 |---> loss is 0.9352480173, total correct predictions: 14257, its 81.006%\n",
      "Epoch:  168 |---> loss is 0.9350289702, total correct predictions: 14262, its 81.034%\n",
      "Epoch:  169 |---> loss is 0.9348377585, total correct predictions: 14264, its 81.045%\n",
      "Epoch:  170 |---> loss is 0.9346545339, total correct predictions: 14265, its 81.051%\n",
      "Epoch:  171 |---> loss is 0.9344647527, total correct predictions: 14269, its 81.074%\n",
      "Epoch:  172 |---> loss is 0.9342764616, total correct predictions: 14270, its 81.080%\n",
      "Epoch:  173 |---> loss is 0.9340941310, total correct predictions: 14273, its 81.097%\n",
      "Epoch:  174 |---> loss is 0.9339255691, total correct predictions: 14275, its 81.108%\n",
      "Epoch:  175 |---> loss is 0.9337750673, total correct predictions: 14276, its 81.114%\n",
      "Epoch:  176 |---> loss is 0.9336126447, total correct predictions: 14276, its 81.114%\n",
      "Epoch:  177 |---> loss is 0.9334469438, total correct predictions: 14278, its 81.125%\n",
      "Epoch:  178 |---> loss is 0.9332866669, total correct predictions: 14281, its 81.142%\n",
      "Epoch:  179 |---> loss is 0.9331312180, total correct predictions: 14281, its 81.142%\n",
      "Epoch:  180 |---> loss is 0.9329739809, total correct predictions: 14283, its 81.153%\n",
      "Epoch:  181 |---> loss is 0.9328179359, total correct predictions: 14284, its 81.159%\n",
      "Epoch:  182 |---> loss is 0.9326723218, total correct predictions: 14287, its 81.176%\n",
      "Epoch:  183 |---> loss is 0.9325181842, total correct predictions: 14288, its 81.182%\n",
      "Epoch:  184 |---> loss is 0.9323812723, total correct predictions: 14289, its 81.188%\n",
      "Epoch:  185 |---> loss is 0.9322277904, total correct predictions: 14291, its 81.199%\n",
      "Epoch:  186 |---> loss is 0.9320674539, total correct predictions: 14295, its 81.222%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  187 |---> loss is 0.9319414496, total correct predictions: 14296, its 81.227%\n",
      "Epoch:  188 |---> loss is 0.9318181872, total correct predictions: 14296, its 81.227%\n",
      "Epoch:  189 |---> loss is 0.9316853285, total correct predictions: 14297, its 81.233%\n",
      "Epoch:  190 |---> loss is 0.9315498471, total correct predictions: 14298, its 81.239%\n",
      "Epoch:  191 |---> loss is 0.9314206839, total correct predictions: 14301, its 81.256%\n",
      "Epoch:  192 |---> loss is 0.9312595725, total correct predictions: 14301, its 81.256%\n",
      "Epoch:  193 |---> loss is 0.9310767651, total correct predictions: 14306, its 81.284%\n",
      "Epoch:  194 |---> loss is 0.9309156537, total correct predictions: 14310, its 81.307%\n",
      "Epoch:  195 |---> loss is 0.9308017492, total correct predictions: 14310, its 81.307%\n",
      "Epoch:  196 |---> loss is 0.9306952357, total correct predictions: 14311, its 81.312%\n",
      "Epoch:  197 |---> loss is 0.9305782318, total correct predictions: 14312, its 81.318%\n",
      "Epoch:  198 |---> loss is 0.9304546714, total correct predictions: 14312, its 81.318%\n",
      "Epoch:  199 |---> loss is 0.9303150177, total correct predictions: 14316, its 81.341%\n",
      "Epoch:  200 |---> loss is 0.9301947951, total correct predictions: 14318, its 81.352%\n",
      "Epoch:  201 |---> loss is 0.9300928712, total correct predictions: 14318, its 81.352%\n",
      "Epoch:  202 |---> loss is 0.9299979806, total correct predictions: 14318, its 81.352%\n",
      "Epoch:  203 |---> loss is 0.9299007058, total correct predictions: 14320, its 81.364%\n",
      "Epoch:  204 |---> loss is 0.9297980666, total correct predictions: 14321, its 81.369%\n",
      "Epoch:  205 |---> loss is 0.9296885133, total correct predictions: 14322, its 81.375%\n",
      "Epoch:  206 |---> loss is 0.9295653105, total correct predictions: 14323, its 81.381%\n",
      "Epoch:  207 |---> loss is 0.9294453263, total correct predictions: 14327, its 81.403%\n",
      "Epoch:  208 |---> loss is 0.9293521047, total correct predictions: 14327, its 81.403%\n",
      "Epoch:  209 |---> loss is 0.9292738438, total correct predictions: 14327, its 81.403%\n",
      "Epoch:  210 |---> loss is 0.9291975498, total correct predictions: 14328, its 81.409%\n",
      "Epoch:  211 |---> loss is 0.9291191101, total correct predictions: 14330, its 81.420%\n",
      "Epoch:  212 |---> loss is 0.9290400147, total correct predictions: 14329, its 81.415%\n",
      "Epoch:  213 |---> loss is 0.9289499521, total correct predictions: 14329, its 81.415%\n",
      "Epoch:  214 |---> loss is 0.9288346767, total correct predictions: 14333, its 81.438%\n",
      "Epoch:  215 |---> loss is 0.9287413955, total correct predictions: 14333, its 81.438%\n",
      "Epoch:  216 |---> loss is 0.9286704063, total correct predictions: 14333, its 81.438%\n",
      "Epoch:  217 |---> loss is 0.9285904169, total correct predictions: 14333, its 81.438%\n",
      "Epoch:  218 |---> loss is 0.9284929037, total correct predictions: 14336, its 81.455%\n",
      "Epoch:  219 |---> loss is 0.9284102321, total correct predictions: 14337, its 81.460%\n",
      "Epoch:  220 |---> loss is 0.9283418655, total correct predictions: 14338, its 81.466%\n",
      "Epoch:  221 |---> loss is 0.9282799959, total correct predictions: 14338, its 81.466%\n",
      "Epoch:  222 |---> loss is 0.9282171130, total correct predictions: 14337, its 81.460%\n",
      "Epoch:  223 |---> loss is 0.9281485677, total correct predictions: 14337, its 81.460%\n",
      "Epoch:  224 |---> loss is 0.9280648232, total correct predictions: 14340, its 81.477%\n",
      "Epoch:  225 |---> loss is 0.9279803634, total correct predictions: 14340, its 81.477%\n",
      "Epoch:  226 |---> loss is 0.9279090762, total correct predictions: 14342, its 81.489%\n",
      "Epoch:  227 |---> loss is 0.9278415442, total correct predictions: 14342, its 81.489%\n",
      "Epoch:  228 |---> loss is 0.9277752042, total correct predictions: 14345, its 81.506%\n",
      "Epoch:  229 |---> loss is 0.9277195334, total correct predictions: 14345, its 81.506%\n",
      "Epoch:  230 |---> loss is 0.9276532531, total correct predictions: 14345, its 81.506%\n",
      "Epoch:  231 |---> loss is 0.9275767803, total correct predictions: 14346, its 81.511%\n",
      "Epoch:  232 |---> loss is 0.9275175929, total correct predictions: 14348, its 81.523%\n",
      "Epoch:  233 |---> loss is 0.9274647832, total correct predictions: 14348, its 81.523%\n",
      "Epoch:  234 |---> loss is 0.9274134040, total correct predictions: 14348, its 81.523%\n",
      "Epoch:  235 |---> loss is 0.9273642898, total correct predictions: 14348, its 81.523%\n",
      "Epoch:  236 |---> loss is 0.9273134470, total correct predictions: 14350, its 81.534%\n",
      "Epoch:  237 |---> loss is 0.9272587895, total correct predictions: 14351, its 81.540%\n",
      "Epoch:  238 |---> loss is 0.9272059202, total correct predictions: 14351, its 81.540%\n",
      "Epoch:  239 |---> loss is 0.9271612167, total correct predictions: 14352, its 81.545%\n",
      "Epoch:  240 |---> loss is 0.9271180630, total correct predictions: 14352, its 81.545%\n",
      "Epoch:  241 |---> loss is 0.9270709753, total correct predictions: 14352, its 81.545%\n",
      "Epoch:  242 |---> loss is 0.9270154834, total correct predictions: 14355, its 81.562%\n",
      "Epoch:  243 |---> loss is 0.9269551635, total correct predictions: 14355, its 81.562%\n",
      "Epoch:  244 |---> loss is 0.9269057512, total correct predictions: 14355, its 81.562%\n",
      "Epoch:  245 |---> loss is 0.9268595576, total correct predictions: 14356, its 81.568%\n",
      "Epoch:  246 |---> loss is 0.9268080592, total correct predictions: 14356, its 81.568%\n",
      "Epoch:  247 |---> loss is 0.9267606139, total correct predictions: 14356, its 81.568%\n",
      "Epoch:  248 |---> loss is 0.9267151356, total correct predictions: 14356, its 81.568%\n",
      "Epoch:  249 |---> loss is 0.9266697168, total correct predictions: 14357, its 81.574%\n",
      "Epoch:  250 |---> loss is 0.9266234040, total correct predictions: 14358, its 81.580%\n",
      "Epoch:  251 |---> loss is 0.9265738726, total correct predictions: 14359, its 81.585%\n",
      "Epoch:  252 |---> loss is 0.9265230298, total correct predictions: 14359, its 81.585%\n",
      "Epoch:  253 |---> loss is 0.9264740944, total correct predictions: 14361, its 81.597%\n",
      "Epoch:  254 |---> loss is 0.9264118075, total correct predictions: 14362, its 81.602%\n",
      "Epoch:  255 |---> loss is 0.9263452291, total correct predictions: 14362, its 81.602%\n",
      "Epoch:  256 |---> loss is 0.9262850881, total correct predictions: 14363, its 81.608%\n",
      "Epoch:  257 |---> loss is 0.9262321591, total correct predictions: 14364, its 81.614%\n",
      "Epoch:  258 |---> loss is 0.9261931181, total correct predictions: 14364, its 81.614%\n",
      "Epoch:  259 |---> loss is 0.9261657000, total correct predictions: 14364, its 81.614%\n",
      "Epoch:  260 |---> loss is 0.9261419773, total correct predictions: 14365, its 81.619%\n",
      "Epoch:  261 |---> loss is 0.9261241555, total correct predictions: 14365, its 81.619%\n",
      "Epoch:  262 |---> loss is 0.9261035919, total correct predictions: 14365, its 81.619%\n",
      "Epoch:  263 |---> loss is 0.9260654449, total correct predictions: 14366, its 81.625%\n",
      "Epoch:  264 |---> loss is 0.9260097742, total correct predictions: 14366, its 81.625%\n",
      "Epoch:  265 |---> loss is 0.9259513021, total correct predictions: 14366, its 81.625%\n",
      "Epoch:  266 |---> loss is 0.9258882403, total correct predictions: 14366, its 81.625%\n",
      "Epoch:  267 |---> loss is 0.9257948399, total correct predictions: 14368, its 81.636%\n",
      "Epoch:  268 |---> loss is 0.9257432818, total correct predictions: 14369, its 81.642%\n",
      "Epoch:  269 |---> loss is 0.9256965518, total correct predictions: 14369, its 81.642%\n",
      "Epoch:  270 |---> loss is 0.9256481528, total correct predictions: 14371, its 81.653%\n",
      "Epoch:  271 |---> loss is 0.9256513715, total correct predictions: 14371, its 81.653%\n",
      "Epoch:  272 |---> loss is 0.9256476760, total correct predictions: 14372, its 81.659%\n",
      "Epoch:  273 |---> loss is 0.9255919456, total correct predictions: 14374, its 81.670%\n",
      "Epoch:  274 |---> loss is 0.9254966378, total correct predictions: 14376, its 81.682%\n",
      "Epoch:  275 |---> loss is 0.9254148602, total correct predictions: 14376, its 81.682%\n",
      "Epoch:  276 |---> loss is 0.9253394604, total correct predictions: 14376, its 81.682%\n",
      "Epoch:  277 |---> loss is 0.9252796173, total correct predictions: 14376, its 81.682%\n",
      "Epoch:  278 |---> loss is 0.9252321124, total correct predictions: 14377, its 81.688%\n",
      "Epoch:  279 |---> loss is 0.9251949787, total correct predictions: 14377, its 81.688%\n",
      "Epoch:  280 |---> loss is 0.9251548052, total correct predictions: 14379, its 81.699%\n",
      "Epoch:  281 |---> loss is 0.9251263142, total correct predictions: 14380, its 81.705%\n",
      "Epoch:  282 |---> loss is 0.9250991344, total correct predictions: 14382, its 81.716%\n",
      "Epoch:  283 |---> loss is 0.9250317812, total correct predictions: 14383, its 81.722%\n",
      "Epoch:  284 |---> loss is 0.9249273539, total correct predictions: 14384, its 81.727%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  285 |---> loss is 0.9248479605, total correct predictions: 14384, its 81.727%\n",
      "Epoch:  286 |---> loss is 0.9247975349, total correct predictions: 14384, its 81.727%\n",
      "Epoch:  287 |---> loss is 0.9247654676, total correct predictions: 14385, its 81.733%\n",
      "Epoch:  288 |---> loss is 0.9247457385, total correct predictions: 14385, its 81.733%\n",
      "Epoch:  289 |---> loss is 0.9247438908, total correct predictions: 14385, its 81.733%\n",
      "Epoch:  290 |---> loss is 0.9247734547, total correct predictions: 14385, its 81.733%\n",
      "Epoch:  291 |---> loss is 0.9248172045, total correct predictions: 14386, its 81.739%\n",
      "Epoch:  292 |---> loss is 0.9247297645, total correct predictions: 14387, its 81.744%\n",
      "Epoch:  293 |---> loss is 0.9246082902, total correct predictions: 14388, its 81.750%\n",
      "Epoch:  294 |---> loss is 0.9245193005, total correct predictions: 14388, its 81.750%\n",
      "Epoch:  295 |---> loss is 0.9244589806, total correct predictions: 14389, its 81.756%\n",
      "Epoch:  296 |---> loss is 0.9244211316, total correct predictions: 14389, its 81.756%\n",
      "Epoch:  297 |---> loss is 0.9244091511, total correct predictions: 14390, its 81.761%\n",
      "Epoch:  298 |---> loss is 0.9244149327, total correct predictions: 14390, its 81.761%\n",
      "Epoch:  299 |---> loss is 0.9244263172, total correct predictions: 14392, its 81.773%\n",
      "Epoch:  300 |---> loss is 0.9243037701, total correct predictions: 14393, its 81.778%\n",
      "Epoch:  301 |---> loss is 0.9241884947, total correct predictions: 14394, its 81.784%\n",
      "Epoch:  302 |---> loss is 0.9240965843, total correct predictions: 14394, its 81.784%\n",
      "Epoch:  303 |---> loss is 0.9239859581, total correct predictions: 14396, its 81.795%\n",
      "Epoch:  304 |---> loss is 0.9239221811, total correct predictions: 14398, its 81.807%\n",
      "Epoch:  305 |---> loss is 0.9239285588, total correct predictions: 14399, its 81.812%\n",
      "Epoch:  306 |---> loss is 0.9239639044, total correct predictions: 14399, its 81.812%\n",
      "Epoch:  307 |---> loss is 0.9240049124, total correct predictions: 14400, its 81.818%\n",
      "Epoch:  308 |---> loss is 0.9238494039, total correct predictions: 14402, its 81.830%\n",
      "Epoch:  309 |---> loss is 0.9236901402, total correct predictions: 14402, its 81.830%\n",
      "Epoch:  310 |---> loss is 0.9236106277, total correct predictions: 14403, its 81.835%\n",
      "Epoch:  311 |---> loss is 0.9235692024, total correct predictions: 14404, its 81.841%\n",
      "Epoch:  312 |---> loss is 0.9235685468, total correct predictions: 14404, its 81.841%\n",
      "Epoch:  313 |---> loss is 0.9236038327, total correct predictions: 14404, its 81.841%\n",
      "Epoch:  314 |---> loss is 0.9236500859, total correct predictions: 14405, its 81.847%\n",
      "Epoch:  315 |---> loss is 0.9235965014, total correct predictions: 14405, its 81.847%\n",
      "Epoch:  316 |---> loss is 0.9234601855, total correct predictions: 14406, its 81.852%\n",
      "Epoch:  317 |---> loss is 0.9233321548, total correct predictions: 14407, its 81.858%\n",
      "Epoch:  318 |---> loss is 0.9232781529, total correct predictions: 14407, its 81.858%\n",
      "Epoch:  319 |---> loss is 0.9232636690, total correct predictions: 14407, its 81.858%\n",
      "Epoch:  320 |---> loss is 0.9232994318, total correct predictions: 14407, its 81.858%\n",
      "Epoch:  321 |---> loss is 0.9234174490, total correct predictions: 14408, its 81.864%\n",
      "Epoch:  322 |---> loss is 0.9233955145, total correct predictions: 14409, its 81.869%\n",
      "Epoch:  323 |---> loss is 0.9232563972, total correct predictions: 14410, its 81.875%\n",
      "Epoch:  324 |---> loss is 0.9231210947, total correct predictions: 14410, its 81.875%\n",
      "Epoch:  325 |---> loss is 0.9230557084, total correct predictions: 14410, its 81.875%\n",
      "Epoch:  326 |---> loss is 0.9230245352, total correct predictions: 14410, its 81.875%\n",
      "Epoch:  327 |---> loss is 0.9230199456, total correct predictions: 14410, its 81.875%\n",
      "Epoch:  328 |---> loss is 0.9230690002, total correct predictions: 14410, its 81.875%\n",
      "Epoch:  329 |---> loss is 0.9232230783, total correct predictions: 14410, its 81.875%\n",
      "Epoch:  330 |---> loss is 0.9232581258, total correct predictions: 14410, its 81.875%\n",
      "Epoch:  331 |---> loss is 0.9230790734, total correct predictions: 14410, its 81.875%\n",
      "Epoch:  332 |---> loss is 0.9229432344, total correct predictions: 14411, its 81.881%\n",
      "Epoch:  333 |---> loss is 0.9228727221, total correct predictions: 14411, its 81.881%\n",
      "Epoch:  334 |---> loss is 0.9228233695, total correct predictions: 14412, its 81.886%\n",
      "Epoch:  335 |---> loss is 0.9228609204, total correct predictions: 14412, its 81.886%\n",
      "Epoch:  336 |---> loss is 0.9229686856, total correct predictions: 14413, its 81.892%\n",
      "Epoch:  337 |---> loss is 0.9230412245, total correct predictions: 14414, its 81.898%\n",
      "Epoch:  338 |---> loss is 0.9229092598, total correct predictions: 14414, its 81.898%\n",
      "Epoch:  339 |---> loss is 0.9227473736, total correct predictions: 14414, its 81.898%\n",
      "Epoch:  340 |---> loss is 0.9226825833, total correct predictions: 14414, its 81.898%\n",
      "Epoch:  341 |---> loss is 0.9226996899, total correct predictions: 14414, its 81.898%\n",
      "Epoch:  342 |---> loss is 0.9228427410, total correct predictions: 14414, its 81.898%\n",
      "Epoch:  343 |---> loss is 0.9230293036, total correct predictions: 14415, its 81.903%\n",
      "Epoch:  344 |---> loss is 0.9227964282, total correct predictions: 14415, its 81.903%\n",
      "Epoch:  345 |---> loss is 0.9226484895, total correct predictions: 14415, its 81.903%\n",
      "Epoch:  346 |---> loss is 0.9225921631, total correct predictions: 14415, its 81.903%\n",
      "Epoch:  347 |---> loss is 0.9225723147, total correct predictions: 14415, its 81.903%\n",
      "Epoch:  348 |---> loss is 0.9226064086, total correct predictions: 14415, its 81.903%\n",
      "Epoch:  349 |---> loss is 0.9227765799, total correct predictions: 14415, its 81.903%\n",
      "Epoch:  350 |---> loss is 0.9229474664, total correct predictions: 14415, its 81.903%\n",
      "Epoch:  351 |---> loss is 0.9226810932, total correct predictions: 14416, its 81.909%\n",
      "Epoch:  352 |---> loss is 0.9225379825, total correct predictions: 14416, its 81.909%\n",
      "Epoch:  353 |---> loss is 0.9224954247, total correct predictions: 14416, its 81.909%\n",
      "Epoch:  354 |---> loss is 0.9225234389, total correct predictions: 14417, its 81.915%\n",
      "Epoch:  355 |---> loss is 0.9227666855, total correct predictions: 14417, its 81.915%\n",
      "Epoch:  356 |---> loss is 0.9226800203, total correct predictions: 14417, its 81.915%\n",
      "Epoch:  357 |---> loss is 0.9224818349, total correct predictions: 14419, its 81.926%\n",
      "Epoch:  358 |---> loss is 0.9223435521, total correct predictions: 14420, its 81.932%\n",
      "Epoch:  359 |---> loss is 0.9222802520, total correct predictions: 14420, its 81.932%\n",
      "Epoch:  360 |---> loss is 0.9222786427, total correct predictions: 14420, its 81.932%\n",
      "Epoch:  361 |---> loss is 0.9223655462, total correct predictions: 14420, its 81.932%\n",
      "Epoch:  362 |---> loss is 0.9225706458, total correct predictions: 14420, its 81.932%\n",
      "Epoch:  363 |---> loss is 0.9225689769, total correct predictions: 14420, its 81.932%\n",
      "Epoch:  364 |---> loss is 0.9223526120, total correct predictions: 14420, its 81.932%\n",
      "Epoch:  365 |---> loss is 0.9222379327, total correct predictions: 14420, its 81.932%\n",
      "Epoch:  366 |---> loss is 0.9222275019, total correct predictions: 14420, its 81.932%\n",
      "Epoch:  367 |---> loss is 0.9223885536, total correct predictions: 14420, its 81.932%\n",
      "Epoch:  368 |---> loss is 0.9226198792, total correct predictions: 14420, its 81.932%\n",
      "Epoch:  369 |---> loss is 0.9223883152, total correct predictions: 14421, its 81.938%\n",
      "Epoch:  370 |---> loss is 0.9221761227, total correct predictions: 14422, its 81.943%\n",
      "Epoch:  371 |---> loss is 0.9221360683, total correct predictions: 14422, its 81.943%\n",
      "Epoch:  372 |---> loss is 0.9222840071, total correct predictions: 14422, its 81.943%\n",
      "Epoch:  373 |---> loss is 0.9225556254, total correct predictions: 14422, its 81.943%\n",
      "Epoch:  374 |---> loss is 0.9222728014, total correct predictions: 14422, its 81.943%\n",
      "Epoch:  375 |---> loss is 0.9220923185, total correct predictions: 14422, its 81.943%\n",
      "Epoch:  376 |---> loss is 0.9220882058, total correct predictions: 14422, its 81.943%\n",
      "Epoch:  377 |---> loss is 0.9222993851, total correct predictions: 14423, its 81.949%\n",
      "Epoch:  378 |---> loss is 0.9224942327, total correct predictions: 14424, its 81.955%\n",
      "Epoch:  379 |---> loss is 0.9220913649, total correct predictions: 14424, its 81.955%\n",
      "Epoch:  380 |---> loss is 0.9219896793, total correct predictions: 14424, its 81.955%\n",
      "Epoch:  381 |---> loss is 0.9222143888, total correct predictions: 14424, its 81.955%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  382 |---> loss is 0.9225037694, total correct predictions: 14424, its 81.955%\n",
      "Epoch:  383 |---> loss is 0.9220480919, total correct predictions: 14424, its 81.955%\n",
      "Epoch:  384 |---> loss is 0.9220292568, total correct predictions: 14424, its 81.955%\n",
      "Epoch:  385 |---> loss is 0.9223544002, total correct predictions: 14424, its 81.955%\n",
      "Epoch:  386 |---> loss is 0.9221518636, total correct predictions: 14424, its 81.955%\n",
      "Epoch:  387 |---> loss is 0.9220026731, total correct predictions: 14424, its 81.955%\n",
      "Epoch:  388 |---> loss is 0.9219155312, total correct predictions: 14424, its 81.955%\n",
      "Epoch:  389 |---> loss is 0.9218752384, total correct predictions: 14424, its 81.955%\n",
      "Epoch:  390 |---> loss is 0.9218657017, total correct predictions: 14424, its 81.955%\n",
      "Epoch:  391 |---> loss is 0.9218624830, total correct predictions: 14425, its 81.960%\n",
      "Epoch:  392 |---> loss is 0.9219514132, total correct predictions: 14425, its 81.960%\n",
      "Epoch:  393 |---> loss is 0.9222447276, total correct predictions: 14425, its 81.960%\n",
      "Epoch:  394 |---> loss is 0.9220790267, total correct predictions: 14426, its 81.966%\n",
      "Epoch:  395 |---> loss is 0.9218481183, total correct predictions: 14426, its 81.966%\n",
      "Epoch:  396 |---> loss is 0.9216982722, total correct predictions: 14427, its 81.972%\n",
      "Epoch:  397 |---> loss is 0.9216592908, total correct predictions: 14429, its 81.983%\n",
      "Epoch:  398 |---> loss is 0.9218472242, total correct predictions: 14429, its 81.983%\n",
      "Epoch:  399 |---> loss is 0.9220525622, total correct predictions: 14431, its 81.994%\n",
      "Epoch:  400 |---> loss is 0.9216860533, total correct predictions: 14432, its 82.000%\n",
      "Epoch:  401 |---> loss is 0.9215295315, total correct predictions: 14432, its 82.000%\n",
      "Epoch:  402 |---> loss is 0.9216428399, total correct predictions: 14433, its 82.006%\n",
      "Epoch:  403 |---> loss is 0.9219964147, total correct predictions: 14433, its 82.006%\n",
      "Epoch:  404 |---> loss is 0.9216620326, total correct predictions: 14433, its 82.006%\n",
      "Epoch:  405 |---> loss is 0.9214972854, total correct predictions: 14434, its 82.011%\n",
      "Epoch:  406 |---> loss is 0.9217134118, total correct predictions: 14434, its 82.011%\n",
      "Epoch:  407 |---> loss is 0.9219074249, total correct predictions: 14434, its 82.011%\n",
      "Epoch:  408 |---> loss is 0.9215483069, total correct predictions: 14434, its 82.011%\n",
      "Epoch:  409 |---> loss is 0.9214216471, total correct predictions: 14435, its 82.017%\n",
      "Epoch:  410 |---> loss is 0.9216285944, total correct predictions: 14435, its 82.017%\n",
      "Epoch:  411 |---> loss is 0.9217643142, total correct predictions: 14435, its 82.017%\n",
      "Epoch:  412 |---> loss is 0.9214662910, total correct predictions: 14436, its 82.023%\n",
      "Epoch:  413 |---> loss is 0.9213088155, total correct predictions: 14436, its 82.023%\n",
      "Epoch:  414 |---> loss is 0.9214296937, total correct predictions: 14437, its 82.028%\n",
      "Epoch:  415 |---> loss is 0.9217630029, total correct predictions: 14437, its 82.028%\n",
      "Epoch:  416 |---> loss is 0.9213119745, total correct predictions: 14438, its 82.034%\n",
      "Epoch:  417 |---> loss is 0.9212654829, total correct predictions: 14438, its 82.034%\n",
      "Epoch:  418 |---> loss is 0.9215897322, total correct predictions: 14438, its 82.034%\n",
      "Epoch:  419 |---> loss is 0.9213976860, total correct predictions: 14438, its 82.034%\n",
      "Epoch:  420 |---> loss is 0.9212220907, total correct predictions: 14438, its 82.034%\n",
      "Epoch:  421 |---> loss is 0.9211543798, total correct predictions: 14438, its 82.034%\n",
      "Epoch:  422 |---> loss is 0.9213618636, total correct predictions: 14438, its 82.034%\n",
      "Epoch:  423 |---> loss is 0.9216784835, total correct predictions: 14438, its 82.034%\n",
      "Epoch:  424 |---> loss is 0.9212592840, total correct predictions: 14438, its 82.034%\n",
      "Epoch:  425 |---> loss is 0.9213390350, total correct predictions: 14438, its 82.034%\n",
      "Epoch:  426 |---> loss is 0.9216142297, total correct predictions: 14438, its 82.034%\n",
      "Epoch:  427 |---> loss is 0.9211546183, total correct predictions: 14439, its 82.040%\n",
      "Epoch:  428 |---> loss is 0.9211090803, total correct predictions: 14439, its 82.040%\n",
      "Epoch:  429 |---> loss is 0.9212703109, total correct predictions: 14439, its 82.040%\n",
      "Epoch:  430 |---> loss is 0.9213258028, total correct predictions: 14439, its 82.040%\n",
      "Epoch:  431 |---> loss is 0.9212451577, total correct predictions: 14439, its 82.040%\n",
      "Epoch:  432 |---> loss is 0.9210829735, total correct predictions: 14439, its 82.040%\n",
      "Epoch:  433 |---> loss is 0.9209839106, total correct predictions: 14439, its 82.040%\n",
      "Epoch:  434 |---> loss is 0.9210346937, total correct predictions: 14439, its 82.040%\n",
      "Epoch:  435 |---> loss is 0.9213127494, total correct predictions: 14439, its 82.040%\n",
      "Epoch:  436 |---> loss is 0.9212418795, total correct predictions: 14440, its 82.045%\n",
      "Epoch:  437 |---> loss is 0.9209839702, total correct predictions: 14440, its 82.045%\n",
      "Epoch:  438 |---> loss is 0.9209276438, total correct predictions: 14440, its 82.045%\n",
      "Epoch:  439 |---> loss is 0.9211713076, total correct predictions: 14440, its 82.045%\n",
      "Epoch:  440 |---> loss is 0.9214387536, total correct predictions: 14440, its 82.045%\n",
      "Epoch:  441 |---> loss is 0.9210023284, total correct predictions: 14440, its 82.045%\n",
      "Epoch:  442 |---> loss is 0.9210781455, total correct predictions: 14440, its 82.045%\n",
      "Epoch:  443 |---> loss is 0.9215867519, total correct predictions: 14440, its 82.045%\n",
      "Epoch:  444 |---> loss is 0.9209428430, total correct predictions: 14441, its 82.051%\n",
      "Epoch:  445 |---> loss is 0.9211845398, total correct predictions: 14441, its 82.051%\n",
      "Epoch:  446 |---> loss is 0.9216114879, total correct predictions: 14441, its 82.051%\n",
      "Epoch:  447 |---> loss is 0.9209415317, total correct predictions: 14441, its 82.051%\n",
      "Epoch:  448 |---> loss is 0.9221348166, total correct predictions: 14441, its 82.051%\n",
      "Epoch:  449 |---> loss is 0.9210600257, total correct predictions: 14441, its 82.051%\n",
      "Epoch:  450 |---> loss is 0.9225122929, total correct predictions: 14442, its 82.057%\n",
      "Epoch:  451 |---> loss is 0.9212666750, total correct predictions: 14442, its 82.057%\n",
      "Epoch:  452 |---> loss is 0.9228262305, total correct predictions: 14442, its 82.057%\n",
      "Epoch:  453 |---> loss is 0.9216382504, total correct predictions: 14442, its 82.057%\n",
      "Epoch:  454 |---> loss is 0.9230069518, total correct predictions: 14442, its 82.057%\n",
      "Epoch:  455 |---> loss is 0.9213647842, total correct predictions: 14442, its 82.057%\n",
      "Epoch:  456 |---> loss is 0.9246240854, total correct predictions: 14441, its 82.051%\n",
      "Epoch:  457 |---> loss is 0.9461313486, total correct predictions: 13983, its 79.449%\n",
      "Epoch:  458 |---> loss is 1.0239751339, total correct predictions: 12628, its 71.750%\n",
      "Epoch:  459 |---> loss is 1.0185171366, total correct predictions: 12744, its 72.409%\n",
      "Epoch:  460 |---> loss is 1.0445580482, total correct predictions: 12263, its 69.676%\n",
      "Epoch:  461 |---> loss is 1.0076794624, total correct predictions: 12941, its 73.528%\n",
      "Epoch:  462 |---> loss is 1.0028172731, total correct predictions: 13027, its 74.017%\n",
      "Epoch:  463 |---> loss is 0.9431193471, total correct predictions: 14105, its 80.142%\n",
      "Epoch:  464 |---> loss is 1.0425472260, total correct predictions: 12301, its 69.892%\n",
      "Epoch:  465 |---> loss is 1.0670659542, total correct predictions: 11890, its 67.557%\n",
      "Epoch:  466 |---> loss is 1.0685329437, total correct predictions: 11867, its 67.426%\n",
      "Epoch:  467 |---> loss is 1.0607452393, total correct predictions: 12009, its 68.233%\n",
      "Epoch:  468 |---> loss is 1.0321782827, total correct predictions: 12495, its 70.994%\n",
      "Epoch:  469 |---> loss is 0.9634498358, total correct predictions: 13736, its 78.045%\n",
      "Epoch:  470 |---> loss is 1.0052046776, total correct predictions: 12977, its 73.733%\n",
      "Epoch:  471 |---> loss is 1.0352196693, total correct predictions: 12435, its 70.653%\n",
      "Epoch:  472 |---> loss is 1.0444637537, total correct predictions: 12252, its 69.614%\n",
      "Epoch:  473 |---> loss is 1.0470129251, total correct predictions: 12226, its 69.466%\n",
      "Epoch:  474 |---> loss is 1.0385000706, total correct predictions: 12386, its 70.375%\n",
      "Epoch:  475 |---> loss is 1.0199484825, total correct predictions: 12713, its 72.233%\n",
      "Epoch:  476 |---> loss is 1.0033866167, total correct predictions: 13016, its 73.955%\n",
      "Epoch:  477 |---> loss is 0.9807953238, total correct predictions: 13404, its 76.159%\n",
      "Epoch:  478 |---> loss is 0.9741258621, total correct predictions: 13534, its 76.898%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  479 |---> loss is 0.9990352988, total correct predictions: 13098, its 74.420%\n",
      "Epoch:  480 |---> loss is 0.9809293747, total correct predictions: 13410, its 76.193%\n",
      "Epoch:  481 |---> loss is 0.9657779336, total correct predictions: 13682, its 77.739%\n",
      "Epoch:  482 |---> loss is 0.9709806442, total correct predictions: 13590, its 77.216%\n",
      "Epoch:  483 |---> loss is 0.9597069025, total correct predictions: 13786, its 78.330%\n",
      "Epoch:  484 |---> loss is 0.9561021924, total correct predictions: 13841, its 78.642%\n",
      "Epoch:  485 |---> loss is 0.9581471086, total correct predictions: 13813, its 78.483%\n",
      "Epoch:  486 |---> loss is 0.9520794153, total correct predictions: 13943, its 79.222%\n",
      "Epoch:  487 |---> loss is 0.9460858703, total correct predictions: 14043, its 79.790%\n",
      "Epoch:  488 |---> loss is 0.9389531016, total correct predictions: 14183, its 80.585%\n",
      "Epoch:  489 |---> loss is 0.9438185692, total correct predictions: 14084, its 80.023%\n",
      "Epoch:  490 |---> loss is 0.9384632707, total correct predictions: 14198, its 80.670%\n",
      "Epoch:  491 |---> loss is 0.9340038896, total correct predictions: 14274, its 81.102%\n",
      "Epoch:  492 |---> loss is 0.9342168570, total correct predictions: 14260, its 81.023%\n",
      "Epoch:  493 |---> loss is 0.9329934716, total correct predictions: 14295, its 81.222%\n",
      "Epoch:  494 |---> loss is 0.9300220013, total correct predictions: 14344, its 81.500%\n",
      "Epoch:  495 |---> loss is 0.9289764762, total correct predictions: 14358, its 81.580%\n",
      "Epoch:  496 |---> loss is 0.9277437925, total correct predictions: 14382, its 81.716%\n",
      "Epoch:  497 |---> loss is 0.9267544150, total correct predictions: 14403, its 81.835%\n",
      "Epoch:  498 |---> loss is 0.9256672859, total correct predictions: 14421, its 81.938%\n",
      "Epoch:  499 |---> loss is 0.9246518016, total correct predictions: 14437, its 82.028%\n"
     ]
    }
   ],
   "source": [
    "net = Network()\n",
    "print(net.out.weight)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "number_of_epoches = 500\n",
    "loss_weights = torch.tensor([1.,1.,1.,1.])\n",
    "\n",
    "\n",
    "total_loss = []\n",
    "total_accuracy = []\n",
    "total_val_loss = []\n",
    "total_val_accuracy = []\n",
    "\n",
    "# Learning process\n",
    "for epoch in range(number_of_epoches):\n",
    "    predicted = net(train_features)\n",
    "    loss = F.cross_entropy(predicted, train_labels, loss_weights)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    total_correct = get_correct_predictions(predicted, train_labels)\n",
    "    print(\"Epoch: {:4d} |---> loss is {:4.10f}, total correct predictions: {:5d}, its {:.3f}%\"\n",
    "      .format(epoch, loss.item(), total_correct, total_correct*100/train_features.shape[0]))\n",
    "    \n",
    "    with torch.no_grad():  # record loss and accuracy info for plots\n",
    "        total_loss.append(loss.item())\n",
    "        total_accuracy.append(total_correct*100/train_features.shape[0])\n",
    "        \n",
    "        test_preds = net(test_features)\n",
    "        total_val_loss.append(F.cross_entropy(test_preds, test_labels, loss_weights).item())\n",
    "        total_val_accuracy.append(get_correct_predictions(test_preds, test_labels)*100/test_features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 59.16%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graphs and evaluation\n",
    "with torch.no_grad():\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(total_accuracy, label='accuracy')\n",
    "    plt.plot(total_val_accuracy, label = 'val_accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0, 100])\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(total_loss, label='loss')\n",
    "    plt.plot(total_val_loss, label = 'val_loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    \n",
    "    test_predictions = net(test_features)\n",
    "    test_correct_predictions = get_correct_predictions(test_predictions, test_labels)\n",
    "    print(\"Test accuracy is {:2.2f}%\" .format(test_correct_predictions * 100 /test_features.shape[0]))\n",
    "    \n",
    "    # Creating of confusion matrix\n",
    "    stacked = torch.stack((test_labels, test_predictions.argmax(dim=1)), dim=1)\n",
    "    # horizontal axis - predicted, vertical - true\n",
    "    confusion_matrix = torch.zeros((number_of_groups,number_of_groups), dtype=torch.int32)      \n",
    "    for row in stacked:\n",
    "       confusion_matrix[row[0].item()][row[1].item()] += 1     # row is target, column - predicted\n",
    "    # print(confusion_matrix)\n",
    "    # print(400 - torch.count_nonzero(test_labels_tensor).item())\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(confusion_matrix, cmap='winter')\n",
    "    \n",
    "    ax.set_xticks(np.arange(confusion_matrix.shape[0]))\n",
    "    ax.set_yticks(np.arange(confusion_matrix.shape[1]))\n",
    "    x_labels = []\n",
    "    y_labels = []\n",
    "    for i in range(confusion_matrix.shape[0]):\n",
    "        x_labels.append('Predicted: '+ str(i + 1))\n",
    "        y_labels.append('Real: ' + str(i + 1))\n",
    "    ax.set_xticklabels(x_labels)\n",
    "    ax.set_yticklabels(y_labels)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    for i in range(confusion_matrix.shape[0]):\n",
    "        for j in range(confusion_matrix.shape[1]):\n",
    "            text = ax.text(j, i, str(round(confusion_matrix[i][j].item()*100/test_features.shape[0], 2)) + '%', ha=\"center\", va=\"center\", size=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15.294]\n",
      " [11.586]\n",
      " [12.508]]\n",
      "[[3]\n",
      " [2]\n",
      " [3]]\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAJyElEQVR4nO3d3Ytd5RmG8fvuGE39pq0tkglNDiQghRoZUiRFaIIlVtEe9CABhUohR4rSgmjP+g+IPShCiFrBVGmjgojVSlWs0KYmMW1NJilpsGSCNkoRNdDE6N2D2SlRxs7ae9aatefh+sHg7A82zya5XGuvmbyvkwhAHV/oewAA7SJqoBiiBoohaqAYogaKOaeLFz3X52W5LujipQFI+o9O6FROeq7HOol6uS7Qt7yxi5cGIGlXfv+5j3H6DRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0U0yhq25tsH7J92PY9XQ8FYHTzRm17QtIvJF0v6UpJW2xf2fVgAEbT5Ei9TtLhJEeSnJL0uKSbux0LwKiaRL1C0tGzbs8M7vsU21tt77a9+yOdbGs+AENq7UJZkm1JppJMLdN5bb0sgCE1ifqYpJVn3Z4c3AdgDDWJ+jVJV9hebftcSZslPd3tWABGNe8iCUlO275d0vOSJiQ9lGR/55MBGEmjlU+SPCvp2Y5nAdACfqMMKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYprs0PGQ7eO231iMgQAsTJMj9S8lbep4DgAtmTfqJK9I+vcizAKgBY1WE23C9lZJWyVpuc5v62UBDIltd4BiuPoNFEPUQDFNfqT1mKQ/Slpje8b2j7ofC8ComuyltWUxBgHQDk6/gWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKKbJGmUrbb9k+4Dt/bbvXIzBAIymyWL+pyX9JMle2xdJ2mP7hSQHOp4NwAiabLvzVpK9g+8/kDQtaUXXgwEYzVDb7theJWmtpF1zPMa2O8AYaHyhzPaFkp6QdFeS9z/7ONvuAOOhUdS2l2k26B1Jnux2JAAL0eTqtyU9KGk6yX3djwRgIZocqddLulXSBtv7Bl/f63guACNqsu3Oq5K8CLMAaAG/UQYUQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxTRYeXG77z7b/Mth252eLMRiA0TRZzP+kpA1JPhwsFfyq7d8m+VPHswEYQZOFByPpw8HNZYOvdDkUgNE1Xcx/wvY+ScclvZBkzm13bO+2vfsjnWx5TABNNYo6ycdJrpI0KWmd7W/M8Ry23QHGwFBXv5O8J+klSZs6mQbAgjW5+n2Z7UsH339R0nWSDnY8F4ARNbn6fbmkR2xPaPZ/Ar9O8ky3YwEYVZOr33/V7J7UAJYAfqMMKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKKZx1IMF/V+3zaKDwBgb5kh9p6TprgYB0I6m2+5MSrpB0vZuxwGwUE2P1PdLulvSJ5/3BPbSAsZDkx06bpR0PMme//c89tICxkOTI/V6STfZflPS45I22H6006kAjGzeqJPcm2QyySpJmyW9mOSWzicDMBJ+Tg0U02SDvP9J8rKklzuZBEArOFIDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQTKPljAYriX4g6WNJp5NMdTkUgNENs0bZd5K829kkAFrB6TdQTNOoI+l3tvfY3jrXE9h2BxgPTU+/v53kmO2vSnrB9sEkr5z9hCTbJG2TpIv9pbQ8J4CGGh2pkxwb/Pe4pKckretyKACja7JB3gW2LzrzvaTvSnqj68EAjKbJ6ffXJD1l+8zzf5XkuU6nAjCyeaNOckTSNxdhFgAt4EdaQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFNMoatuX2t5p+6DtadvXdD0YgNE0Xff755KeS/ID2+dKOr/DmQAswLxR275E0rWSfihJSU5JOtXtWABG1eT0e7WkdyQ9bPt129sH639/CtvuAOOhSdTnSLpa0gNJ1ko6Iemezz4pybYkU0mmlum8lscE0FSTqGckzSTZNbi9U7ORAxhD80ad5G1JR22vGdy1UdKBTqcCMLKmV7/vkLRjcOX7iKTbuhsJwEI0ijrJPklT3Y4CoA38RhlQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFzBu17TW295319b7tuxZhNgAjmHeNsiSHJF0lSbYnJB2T9FS3YwEY1bCn3xsl/SPJP7sYBsDCNV0i+IzNkh6b6wHbWyVtlaTl7J8H9KbxkXqw5vdNkn4z1+NsuwOMh2FOv6+XtDfJv7oaBsDCDRP1Fn3OqTeA8dEo6sHWtddJerLbcQAsVNNtd05I+nLHswBoAb9RBhRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxTtL+i9rvSBr2n2d+RdK7rQ8zHqq+N95Xf76e5LK5Hugk6lHY3p1kqu85ulD1vfG+xhOn30AxRA0UM05Rb+t7gA5VfW+8rzE0Np+pAbRjnI7UAFpA1EAxYxG17U22D9k+bPuevudpg+2Vtl+yfcD2ftt39j1Tm2xP2H7d9jN9z9Im25fa3mn7oO1p29f0PdOwev9MPdgg4O+aXS5pRtJrkrYkOdDrYAtk+3JJlyfZa/siSXskfX+pv68zbP9Y0pSki5Pc2Pc8bbH9iKQ/JNk+WEH3/CTv9TzWUMbhSL1O0uEkR5KckvS4pJt7nmnBkryVZO/g+w8kTUta0e9U7bA9KekGSdv7nqVNti+RdK2kByUpyamlFrQ0HlGvkHT0rNszKvKX/wzbqyStlbSr51Hacr+kuyV90vMcbVst6R1JDw8+WmwfLLq5pIxD1KXZvlDSE5LuSvJ+3/MslO0bJR1PsqfvWTpwjqSrJT2QZK2kE5KW3DWecYj6mKSVZ92eHNy35NleptmgdySpsrzyekk32X5Tsx+VNth+tN+RWjMjaSbJmTOqnZqNfEkZh6hfk3SF7dWDCxObJT3d80wLZtua/Ww2neS+vudpS5J7k0wmWaXZP6sXk9zS81itSPK2pKO21wzu2ihpyV3YHHaDvNYlOW37dknPS5qQ9FCS/T2P1Yb1km6V9Dfb+wb3/TTJs/2NhAbukLRjcIA5Ium2nucZWu8/0gLQrnE4/QbQIqIGiiFqoBiiBoohaqAYogaKIWqgmP8C26dZD/wVjtYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convolutional NN\n",
    "features_reshaped = np.reshape(features, (features.shape[0], n_detectors, n_angles, n_radii))\n",
    "\n",
    "train_features_c = torch.tensor(features_reshaped[:size_of_training_set], dtype=torch.float32)\n",
    "test_features_c = torch.tensor(features_reshaped[size_of_training_set:], dtype=torch.float32)\n",
    "\n",
    "\n",
    "print(labels[:3])\n",
    "print(labels_encoded_equisized[:3])\n",
    "\n",
    "# Equisized\n",
    "train_labels_c = torch.flatten(torch.tensor(labels_encoded_equisized[:size_of_training_set]))\n",
    "test_labels_c = torch.flatten(torch.tensor(labels_encoded_equisized[size_of_training_set:]))\n",
    "\n",
    "# print(train_features_conv.shape)\n",
    "print(train_features_c[0][0])\n",
    "plt.imshow(train_features_c[0][0])\n",
    "plt.show()\n",
    "# print(train_features_c[0][1])\n",
    "# plt.imshow(train_features_c[0][1])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional NN\n",
    "class NetworkConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetworkConv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=n_detectors, out_channels=n_detectors*2, kernel_size=(4, 2))\n",
    "        # self.conv1 = nn.Conv2d(in_channels=n_detectors, out_channels=n_detectors*2, kernel_size=(3, 3))\n",
    "        nn.init.normal_(self.conv1.weight, mean=0.0, std=0.02)\n",
    "        # print(self.conv1.weight)\n",
    "        # self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=2)\n",
    "        \n",
    "        # self.lin1 = nn.Linear(in_features=n_detectors*2*5*3, out_features=60)\n",
    "        self.lin1 = nn.Linear(in_features=n_detectors*2*5*7, out_features=60)\n",
    "        nn.init.normal_(self.lin1.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "        self.lin2 = nn.Linear(in_features=60, out_features=16)\n",
    "        nn.init.normal_(self.lin2.weight, mean=0.0, std=0.02)\n",
    "\n",
    "        # self.lin3 = nn.Linear(in_features=30, out_features=12)\n",
    "        # nn.init.normal_(self.lin3.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "        # self.lin4 = nn.Linear(in_features=30, out_features=16)\n",
    "        # nn.init.normal_(self.lin4.weight, mean=0.0, std=0.02) \n",
    "        \n",
    "        self.out = nn.Linear(in_features=16, out_features=number_of_groups)\n",
    "        nn.init.normal_(self.out.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        t = t\n",
    "        t = F.relu(self.conv1(t))\n",
    "        # t = F.relu(self.conv2(t))\n",
    "        # t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        t = F.relu(self.lin1(t.reshape(-1, n_detectors*2*5*7)))\n",
    "        t = F.relu(self.lin2(t))\n",
    "        # t = F.relu(self.lin3(t))\n",
    "        # t = F.relu(self.lin4(t))\n",
    "        \n",
    "        t = F.softmax(self.out(t), dim=1)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 0.0163,  0.0148],\n",
      "          [-0.0001,  0.0349],\n",
      "          [-0.0126, -0.0072],\n",
      "          [-0.0356,  0.0045]],\n",
      "\n",
      "         [[ 0.0299, -0.0070],\n",
      "          [ 0.0152, -0.0137],\n",
      "          [-0.0051,  0.0011],\n",
      "          [ 0.0026, -0.0019]],\n",
      "\n",
      "         [[-0.0017,  0.0051],\n",
      "          [ 0.0178, -0.0065],\n",
      "          [-0.0123, -0.0084],\n",
      "          [-0.0158, -0.0187]],\n",
      "\n",
      "         [[-0.0270, -0.0101],\n",
      "          [ 0.0093,  0.0274],\n",
      "          [ 0.0004,  0.0169],\n",
      "          [-0.0066,  0.0121]],\n",
      "\n",
      "         [[ 0.0128,  0.0160],\n",
      "          [-0.0105,  0.0221],\n",
      "          [ 0.0041,  0.0419],\n",
      "          [-0.0033,  0.0147]],\n",
      "\n",
      "         [[-0.0542, -0.0341],\n",
      "          [ 0.0087,  0.0163],\n",
      "          [-0.0036,  0.0149],\n",
      "          [-0.0092,  0.0006]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0185,  0.0301],\n",
      "          [ 0.0237,  0.0225],\n",
      "          [ 0.0242,  0.0139],\n",
      "          [-0.0331,  0.0129]],\n",
      "\n",
      "         [[-0.0154,  0.0554],\n",
      "          [-0.0318, -0.0094],\n",
      "          [ 0.0053, -0.0108],\n",
      "          [-0.0157, -0.0257]],\n",
      "\n",
      "         [[-0.0039, -0.0239],\n",
      "          [-0.0155,  0.0250],\n",
      "          [ 0.0206, -0.0154],\n",
      "          [-0.0235, -0.0137]],\n",
      "\n",
      "         [[-0.0387, -0.0239],\n",
      "          [-0.0075, -0.0419],\n",
      "          [-0.0074, -0.0048],\n",
      "          [-0.0066, -0.0273]],\n",
      "\n",
      "         [[ 0.0198, -0.0267],\n",
      "          [ 0.0130,  0.0267],\n",
      "          [-0.0112,  0.0068],\n",
      "          [ 0.0464, -0.0078]],\n",
      "\n",
      "         [[ 0.0238, -0.0159],\n",
      "          [-0.0234, -0.0152],\n",
      "          [ 0.0166,  0.0104],\n",
      "          [ 0.0124, -0.0097]]],\n",
      "\n",
      "\n",
      "        [[[-0.0060,  0.0342],\n",
      "          [ 0.0193,  0.0122],\n",
      "          [-0.0044, -0.0113],\n",
      "          [-0.0193,  0.0252]],\n",
      "\n",
      "         [[-0.0043,  0.0119],\n",
      "          [-0.0131,  0.0072],\n",
      "          [-0.0453, -0.0073],\n",
      "          [-0.0201,  0.0053]],\n",
      "\n",
      "         [[ 0.0108, -0.0165],\n",
      "          [-0.0058, -0.0119],\n",
      "          [ 0.0224,  0.0230],\n",
      "          [-0.0171,  0.0107]],\n",
      "\n",
      "         [[ 0.0337, -0.0148],\n",
      "          [-0.0081,  0.0061],\n",
      "          [-0.0091, -0.0166],\n",
      "          [ 0.0005,  0.0154]],\n",
      "\n",
      "         [[ 0.0221, -0.0006],\n",
      "          [-0.0069, -0.0143],\n",
      "          [ 0.0043,  0.0035],\n",
      "          [ 0.0014, -0.0355]],\n",
      "\n",
      "         [[-0.0120, -0.0163],\n",
      "          [ 0.0548, -0.0366],\n",
      "          [ 0.0056,  0.0077],\n",
      "          [-0.0239, -0.0119]]],\n",
      "\n",
      "\n",
      "        [[[-0.0147, -0.0306],\n",
      "          [-0.0270, -0.0077],\n",
      "          [-0.0255, -0.0131],\n",
      "          [-0.0050, -0.0163]],\n",
      "\n",
      "         [[ 0.0181, -0.0267],\n",
      "          [ 0.0474,  0.0263],\n",
      "          [ 0.0100, -0.0300],\n",
      "          [ 0.0058,  0.0310]],\n",
      "\n",
      "         [[ 0.0352, -0.0088],\n",
      "          [ 0.0057, -0.0085],\n",
      "          [ 0.0004,  0.0087],\n",
      "          [ 0.0105, -0.0022]],\n",
      "\n",
      "         [[-0.0005,  0.0159],\n",
      "          [ 0.0119, -0.0193],\n",
      "          [ 0.0129,  0.0002],\n",
      "          [ 0.0061,  0.0249]],\n",
      "\n",
      "         [[ 0.0076, -0.0080],\n",
      "          [-0.0217, -0.0345],\n",
      "          [-0.0085, -0.0112],\n",
      "          [-0.0181,  0.0025]],\n",
      "\n",
      "         [[ 0.0077, -0.0177],\n",
      "          [-0.0355,  0.0327],\n",
      "          [ 0.0234, -0.0066],\n",
      "          [ 0.0240, -0.0191]]],\n",
      "\n",
      "\n",
      "        [[[-0.0068,  0.0007],\n",
      "          [-0.0298, -0.0030],\n",
      "          [ 0.0185,  0.0166],\n",
      "          [ 0.0263, -0.0100]],\n",
      "\n",
      "         [[ 0.0068,  0.0239],\n",
      "          [ 0.0054, -0.0360],\n",
      "          [-0.0275,  0.0135],\n",
      "          [ 0.0017, -0.0218]],\n",
      "\n",
      "         [[-0.0145,  0.0049],\n",
      "          [-0.0358,  0.0078],\n",
      "          [-0.0086,  0.0207],\n",
      "          [-0.0127,  0.0085]],\n",
      "\n",
      "         [[ 0.0104, -0.0354],\n",
      "          [ 0.0470,  0.0145],\n",
      "          [-0.0025,  0.0228],\n",
      "          [-0.0209,  0.0447]],\n",
      "\n",
      "         [[-0.0099,  0.0218],\n",
      "          [-0.0132, -0.0064],\n",
      "          [ 0.0108, -0.0439],\n",
      "          [ 0.0031, -0.0141]],\n",
      "\n",
      "         [[-0.0071,  0.0047],\n",
      "          [-0.0112,  0.0279],\n",
      "          [ 0.0131,  0.0114],\n",
      "          [-0.0097, -0.0236]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0167, -0.0025],\n",
      "          [ 0.0281,  0.0374],\n",
      "          [ 0.0051,  0.0222],\n",
      "          [-0.0346, -0.0114]],\n",
      "\n",
      "         [[ 0.0101,  0.0026],\n",
      "          [-0.0444, -0.0102],\n",
      "          [-0.0074,  0.0093],\n",
      "          [ 0.0209,  0.0086]],\n",
      "\n",
      "         [[-0.0222,  0.0036],\n",
      "          [-0.0002,  0.0163],\n",
      "          [-0.0314,  0.0236],\n",
      "          [-0.0131,  0.0041]],\n",
      "\n",
      "         [[ 0.0029,  0.0038],\n",
      "          [ 0.0269, -0.0005],\n",
      "          [ 0.0006,  0.0198],\n",
      "          [-0.0144, -0.0337]],\n",
      "\n",
      "         [[-0.0085,  0.0372],\n",
      "          [-0.0041,  0.0023],\n",
      "          [-0.0411, -0.0179],\n",
      "          [-0.0231,  0.0014]],\n",
      "\n",
      "         [[-0.0030,  0.0275],\n",
      "          [-0.0210,  0.0203],\n",
      "          [-0.0106, -0.0208],\n",
      "          [-0.0242,  0.0407]]],\n",
      "\n",
      "\n",
      "        [[[-0.0049, -0.0109],\n",
      "          [-0.0146,  0.0384],\n",
      "          [ 0.0256,  0.0094],\n",
      "          [-0.0116, -0.0072]],\n",
      "\n",
      "         [[ 0.0087,  0.0014],\n",
      "          [ 0.0001, -0.0087],\n",
      "          [-0.0076,  0.0139],\n",
      "          [-0.0061,  0.0112]],\n",
      "\n",
      "         [[-0.0236,  0.0155],\n",
      "          [ 0.0338, -0.0076],\n",
      "          [ 0.0102, -0.0022],\n",
      "          [-0.0198,  0.0023]],\n",
      "\n",
      "         [[ 0.0116,  0.0097],\n",
      "          [ 0.0061,  0.0291],\n",
      "          [ 0.0085,  0.0307],\n",
      "          [-0.0109,  0.0178]],\n",
      "\n",
      "         [[ 0.0124, -0.0009],\n",
      "          [ 0.0065, -0.0137],\n",
      "          [-0.0133, -0.0127],\n",
      "          [ 0.0151, -0.0267]],\n",
      "\n",
      "         [[ 0.0276, -0.0163],\n",
      "          [-0.0058,  0.0117],\n",
      "          [ 0.0118,  0.0442],\n",
      "          [-0.0338, -0.0120]]],\n",
      "\n",
      "\n",
      "        [[[-0.0003, -0.0243],\n",
      "          [ 0.0259,  0.0031],\n",
      "          [ 0.0190,  0.0020],\n",
      "          [-0.0056, -0.0093]],\n",
      "\n",
      "         [[ 0.0062, -0.0121],\n",
      "          [-0.0061, -0.0034],\n",
      "          [ 0.0382,  0.0119],\n",
      "          [-0.0045, -0.0108]],\n",
      "\n",
      "         [[ 0.0052, -0.0030],\n",
      "          [-0.0441, -0.0216],\n",
      "          [ 0.0128, -0.0051],\n",
      "          [-0.0107, -0.0066]],\n",
      "\n",
      "         [[ 0.0062,  0.0061],\n",
      "          [-0.0188, -0.0550],\n",
      "          [-0.0031, -0.0101],\n",
      "          [-0.0155,  0.0144]],\n",
      "\n",
      "         [[ 0.0067,  0.0067],\n",
      "          [ 0.0077,  0.0063],\n",
      "          [-0.0016, -0.0405],\n",
      "          [ 0.0097,  0.0186]],\n",
      "\n",
      "         [[-0.0003,  0.0063],\n",
      "          [ 0.0035, -0.0465],\n",
      "          [-0.0103,  0.0151],\n",
      "          [ 0.0384,  0.0160]]],\n",
      "\n",
      "\n",
      "        [[[-0.0423, -0.0489],\n",
      "          [-0.0277,  0.0040],\n",
      "          [-0.0325,  0.0121],\n",
      "          [-0.0034,  0.0067]],\n",
      "\n",
      "         [[-0.0084,  0.0244],\n",
      "          [-0.0091,  0.0031],\n",
      "          [-0.0177, -0.0179],\n",
      "          [ 0.0090,  0.0269]],\n",
      "\n",
      "         [[ 0.0259, -0.0216],\n",
      "          [-0.0216,  0.0241],\n",
      "          [ 0.0104, -0.0078],\n",
      "          [-0.0128, -0.0024]],\n",
      "\n",
      "         [[ 0.0049, -0.0156],\n",
      "          [-0.0018,  0.0050],\n",
      "          [ 0.0295, -0.0197],\n",
      "          [-0.0079, -0.0178]],\n",
      "\n",
      "         [[-0.0140,  0.0100],\n",
      "          [ 0.0214, -0.0030],\n",
      "          [ 0.0414,  0.0008],\n",
      "          [-0.0100,  0.0099]],\n",
      "\n",
      "         [[ 0.0028, -0.0106],\n",
      "          [ 0.0223, -0.0333],\n",
      "          [ 0.0204, -0.0033],\n",
      "          [-0.0160,  0.0238]]],\n",
      "\n",
      "\n",
      "        [[[-0.0405,  0.0040],\n",
      "          [ 0.0113, -0.0088],\n",
      "          [-0.0086,  0.0093],\n",
      "          [-0.0055,  0.0014]],\n",
      "\n",
      "         [[-0.0388,  0.0129],\n",
      "          [ 0.0063, -0.0302],\n",
      "          [-0.0187, -0.0096],\n",
      "          [ 0.0008,  0.0236]],\n",
      "\n",
      "         [[ 0.0104, -0.0214],\n",
      "          [ 0.0166, -0.0141],\n",
      "          [ 0.0085,  0.0085],\n",
      "          [ 0.0089,  0.0003]],\n",
      "\n",
      "         [[-0.0121, -0.0016],\n",
      "          [-0.0072, -0.0360],\n",
      "          [-0.0216, -0.0082],\n",
      "          [ 0.0043,  0.0169]],\n",
      "\n",
      "         [[-0.0194,  0.0214],\n",
      "          [ 0.0254,  0.0254],\n",
      "          [-0.0042,  0.0030],\n",
      "          [-0.0118,  0.0083]],\n",
      "\n",
      "         [[-0.0165, -0.0013],\n",
      "          [-0.0245, -0.0068],\n",
      "          [-0.0168,  0.0007],\n",
      "          [-0.0006, -0.0241]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0072,  0.0088],\n",
      "          [-0.0156, -0.0161],\n",
      "          [ 0.0221, -0.0264],\n",
      "          [ 0.0202,  0.0007]],\n",
      "\n",
      "         [[ 0.0090, -0.0057],\n",
      "          [-0.0517,  0.0122],\n",
      "          [ 0.0294, -0.0204],\n",
      "          [ 0.0009,  0.0065]],\n",
      "\n",
      "         [[ 0.0038,  0.0105],\n",
      "          [ 0.0315, -0.0072],\n",
      "          [-0.0040, -0.0036],\n",
      "          [-0.0264, -0.0088]],\n",
      "\n",
      "         [[-0.0189,  0.0158],\n",
      "          [ 0.0078, -0.0027],\n",
      "          [-0.0096, -0.0125],\n",
      "          [-0.0012, -0.0213]],\n",
      "\n",
      "         [[-0.0102, -0.0234],\n",
      "          [-0.0059, -0.0124],\n",
      "          [-0.0048,  0.0371],\n",
      "          [ 0.0026, -0.0085]],\n",
      "\n",
      "         [[-0.0183, -0.0234],\n",
      "          [-0.0030,  0.0012],\n",
      "          [-0.0096,  0.0250],\n",
      "          [ 0.0010, -0.0296]]],\n",
      "\n",
      "\n",
      "        [[[-0.0063,  0.0489],\n",
      "          [-0.0057, -0.0229],\n",
      "          [-0.0134,  0.0318],\n",
      "          [ 0.0087, -0.0099]],\n",
      "\n",
      "         [[-0.0166,  0.0035],\n",
      "          [-0.0215, -0.0264],\n",
      "          [ 0.0059, -0.0174],\n",
      "          [ 0.0081,  0.0114]],\n",
      "\n",
      "         [[ 0.0085,  0.0212],\n",
      "          [ 0.0062,  0.0046],\n",
      "          [-0.0225,  0.0085],\n",
      "          [-0.0281,  0.0169]],\n",
      "\n",
      "         [[ 0.0330, -0.0243],\n",
      "          [-0.0264, -0.0192],\n",
      "          [ 0.0015,  0.0438],\n",
      "          [-0.0405, -0.0044]],\n",
      "\n",
      "         [[-0.0251,  0.0003],\n",
      "          [ 0.0190, -0.0046],\n",
      "          [ 0.0142, -0.0044],\n",
      "          [-0.0055,  0.0225]],\n",
      "\n",
      "         [[ 0.0182,  0.0242],\n",
      "          [ 0.0165,  0.0352],\n",
      "          [ 0.0521,  0.0336],\n",
      "          [-0.0399,  0.0129]]]], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0 |---> loss is 1.3874046803, total correct predictions:  4370, its 24.830%\n",
      "Epoch:    1 |---> loss is 1.3870083094, total correct predictions:  4370, its 24.830%\n",
      "Epoch:    2 |---> loss is 1.3827983141, total correct predictions:  4370, its 24.830%\n",
      "Epoch:    3 |---> loss is 1.3654382229, total correct predictions:  4391, its 24.949%\n",
      "Epoch:    4 |---> loss is 1.3306127787, total correct predictions:  5165, its 29.347%\n",
      "Epoch:    5 |---> loss is 1.3373343945, total correct predictions:  4578, its 26.011%\n",
      "Epoch:    6 |---> loss is 1.3303210735, total correct predictions:  4663, its 26.494%\n",
      "Epoch:    7 |---> loss is 1.3169960976, total correct predictions:  7164, its 40.705%\n",
      "Epoch:    8 |---> loss is 1.3162353039, total correct predictions:  7984, its 45.364%\n",
      "Epoch:    9 |---> loss is 1.3150024414, total correct predictions:  8660, its 49.205%\n",
      "Epoch:   10 |---> loss is 1.3015381098, total correct predictions:  8424, its 47.864%\n",
      "Epoch:   11 |---> loss is 1.2999536991, total correct predictions:  8010, its 45.511%\n",
      "Epoch:   12 |---> loss is 1.2989087105, total correct predictions:  8172, its 46.432%\n",
      "Epoch:   13 |---> loss is 1.2898975611, total correct predictions:  8397, its 47.710%\n",
      "Epoch:   14 |---> loss is 1.2857450247, total correct predictions:  8554, its 48.602%\n",
      "Epoch:   15 |---> loss is 1.2840150595, total correct predictions:  8491, its 48.244%\n",
      "Epoch:   16 |---> loss is 1.2744358778, total correct predictions:  8508, its 48.341%\n",
      "Epoch:   17 |---> loss is 1.2704524994, total correct predictions:  8479, its 48.176%\n",
      "Epoch:   18 |---> loss is 1.2655308247, total correct predictions:  8486, its 48.216%\n",
      "Epoch:   19 |---> loss is 1.2565494776, total correct predictions:  8610, its 48.920%\n",
      "Epoch:   20 |---> loss is 1.2509104013, total correct predictions:  8451, its 48.017%\n",
      "Epoch:   21 |---> loss is 1.2441861629, total correct predictions:  8415, its 47.812%\n",
      "Epoch:   22 |---> loss is 1.2354186773, total correct predictions:  8508, its 48.341%\n",
      "Epoch:   23 |---> loss is 1.2311184406, total correct predictions:  8555, its 48.608%\n",
      "Epoch:   24 |---> loss is 1.2242053747, total correct predictions:  8631, its 49.040%\n",
      "Epoch:   25 |---> loss is 1.2193955183, total correct predictions:  8623, its 48.994%\n",
      "Epoch:   26 |---> loss is 1.2155078650, total correct predictions:  8711, its 49.494%\n",
      "Epoch:   27 |---> loss is 1.2103161812, total correct predictions:  8848, its 50.273%\n",
      "Epoch:   28 |---> loss is 1.2072201967, total correct predictions:  8981, its 51.028%\n",
      "Epoch:   29 |---> loss is 1.2001236677, total correct predictions:  9187, its 52.199%\n",
      "Epoch:   30 |---> loss is 1.1951589584, total correct predictions:  9409, its 53.460%\n",
      "Epoch:   31 |---> loss is 1.1859726906, total correct predictions:  9755, its 55.426%\n",
      "Epoch:   32 |---> loss is 1.1749805212, total correct predictions:  9934, its 56.443%\n",
      "Epoch:   33 |---> loss is 1.1664136648, total correct predictions: 10291, its 58.472%\n",
      "Epoch:   34 |---> loss is 1.1584364176, total correct predictions: 10124, its 57.523%\n",
      "Epoch:   35 |---> loss is 1.1569634676, total correct predictions: 10154, its 57.693%\n",
      "Epoch:   36 |---> loss is 1.1471862793, total correct predictions: 10206, its 57.989%\n",
      "Epoch:   37 |---> loss is 1.1543718576, total correct predictions:  9484, its 53.886%\n",
      "Epoch:   38 |---> loss is 1.1478677988, total correct predictions: 10130, its 57.557%\n",
      "Epoch:   39 |---> loss is 1.1482946873, total correct predictions: 10216, its 58.045%\n",
      "Epoch:   40 |---> loss is 1.1505842209, total correct predictions: 10199, its 57.949%\n",
      "Epoch:   41 |---> loss is 1.1482192278, total correct predictions: 10192, its 57.909%\n",
      "Epoch:   42 |---> loss is 1.1380223036, total correct predictions: 10290, its 58.466%\n",
      "Epoch:   43 |---> loss is 1.1548728943, total correct predictions:  9506, its 54.011%\n",
      "Epoch:   44 |---> loss is 1.1343219280, total correct predictions: 10410, its 59.148%\n",
      "Epoch:   45 |---> loss is 1.1400012970, total correct predictions: 10256, its 58.273%\n",
      "Epoch:   46 |---> loss is 1.1323175430, total correct predictions: 10392, its 59.045%\n",
      "Epoch:   47 |---> loss is 1.1380245686, total correct predictions: 10459, its 59.426%\n",
      "Epoch:   48 |---> loss is 1.1294041872, total correct predictions: 10532, its 59.841%\n",
      "Epoch:   49 |---> loss is 1.1349745989, total correct predictions: 10432, its 59.273%\n",
      "Epoch:   50 |---> loss is 1.1290132999, total correct predictions: 10638, its 60.443%\n",
      "Epoch:   51 |---> loss is 1.1285340786, total correct predictions: 10628, its 60.386%\n",
      "Epoch:   52 |---> loss is 1.1278673410, total correct predictions: 10498, its 59.648%\n",
      "Epoch:   53 |---> loss is 1.1279616356, total correct predictions: 10509, its 59.710%\n",
      "Epoch:   54 |---> loss is 1.1234217882, total correct predictions: 10655, its 60.540%\n",
      "Epoch:   55 |---> loss is 1.1255533695, total correct predictions: 10647, its 60.494%\n",
      "Epoch:   56 |---> loss is 1.1206361055, total correct predictions: 10823, its 61.494%\n",
      "Epoch:   57 |---> loss is 1.1210094690, total correct predictions: 10752, its 61.091%\n",
      "Epoch:   58 |---> loss is 1.1190328598, total correct predictions: 10827, its 61.517%\n",
      "Epoch:   59 |---> loss is 1.1184411049, total correct predictions: 10834, its 61.557%\n",
      "Epoch:   60 |---> loss is 1.1156312227, total correct predictions: 10906, its 61.966%\n",
      "Epoch:   61 |---> loss is 1.1164522171, total correct predictions: 10892, its 61.886%\n",
      "Epoch:   62 |---> loss is 1.1130610704, total correct predictions: 10940, its 62.159%\n",
      "Epoch:   63 |---> loss is 1.1138149500, total correct predictions: 10889, its 61.869%\n",
      "Epoch:   64 |---> loss is 1.1113708019, total correct predictions: 10972, its 62.341%\n",
      "Epoch:   65 |---> loss is 1.1099503040, total correct predictions: 11032, its 62.682%\n",
      "Epoch:   66 |---> loss is 1.1101443768, total correct predictions: 11009, its 62.551%\n",
      "Epoch:   67 |---> loss is 1.1069493294, total correct predictions: 11079, its 62.949%\n",
      "Epoch:   68 |---> loss is 1.1066540480, total correct predictions: 11075, its 62.926%\n",
      "Epoch:   69 |---> loss is 1.1054100990, total correct predictions: 11114, its 63.148%\n",
      "Epoch:   70 |---> loss is 1.1043431759, total correct predictions: 11115, its 63.153%\n",
      "Epoch:   71 |---> loss is 1.1021181345, total correct predictions: 11176, its 63.500%\n",
      "Epoch:   72 |---> loss is 1.1021984816, total correct predictions: 11155, its 63.381%\n",
      "Epoch:   73 |---> loss is 1.1010011435, total correct predictions: 11205, its 63.665%\n",
      "Epoch:   74 |---> loss is 1.1006815434, total correct predictions: 11180, its 63.523%\n",
      "Epoch:   75 |---> loss is 1.0991716385, total correct predictions: 11238, its 63.852%\n",
      "Epoch:   76 |---> loss is 1.0983314514, total correct predictions: 11230, its 63.807%\n",
      "Epoch:   77 |---> loss is 1.0963134766, total correct predictions: 11279, its 64.085%\n",
      "Epoch:   78 |---> loss is 1.0953384638, total correct predictions: 11311, its 64.267%\n",
      "Epoch:   79 |---> loss is 1.0946276188, total correct predictions: 11346, its 64.466%\n",
      "Epoch:   80 |---> loss is 1.0934710503, total correct predictions: 11376, its 64.636%\n",
      "Epoch:   81 |---> loss is 1.0946524143, total correct predictions: 11337, its 64.415%\n",
      "Epoch:   82 |---> loss is 1.0957623720, total correct predictions: 11302, its 64.216%\n",
      "Epoch:   83 |---> loss is 1.1026835442, total correct predictions: 11140, its 63.295%\n",
      "Epoch:   84 |---> loss is 1.0923706293, total correct predictions: 11390, its 64.716%\n",
      "Epoch:   85 |---> loss is 1.0912381411, total correct predictions: 11431, its 64.949%\n",
      "Epoch:   86 |---> loss is 1.0925703049, total correct predictions: 11358, its 64.534%\n",
      "Epoch:   87 |---> loss is 1.0979896784, total correct predictions: 11224, its 63.773%\n",
      "Epoch:   88 |---> loss is 1.0907405615, total correct predictions: 11419, its 64.881%\n",
      "Epoch:   89 |---> loss is 1.0881689787, total correct predictions: 11497, its 65.324%\n",
      "Epoch:   90 |---> loss is 1.0881344080, total correct predictions: 11478, its 65.216%\n",
      "Epoch:   91 |---> loss is 1.0889042616, total correct predictions: 11444, its 65.023%\n",
      "Epoch:   92 |---> loss is 1.0859336853, total correct predictions: 11537, its 65.551%\n",
      "Epoch:   93 |---> loss is 1.0830279589, total correct predictions: 11602, its 65.920%\n",
      "Epoch:   94 |---> loss is 1.0839338303, total correct predictions: 11549, its 65.619%\n",
      "Epoch:   95 |---> loss is 1.0829814672, total correct predictions: 11587, its 65.835%\n",
      "Epoch:   96 |---> loss is 1.0838999748, total correct predictions: 11573, its 65.756%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   97 |---> loss is 1.0824674368, total correct predictions: 11591, its 65.858%\n",
      "Epoch:   98 |---> loss is 1.0818573236, total correct predictions: 11596, its 65.886%\n",
      "Epoch:   99 |---> loss is 1.0800544024, total correct predictions: 11656, its 66.227%\n",
      "Epoch:  100 |---> loss is 1.0787035227, total correct predictions: 11696, its 66.455%\n",
      "Epoch:  101 |---> loss is 1.0782397985, total correct predictions: 11720, its 66.591%\n",
      "Epoch:  102 |---> loss is 1.0771737099, total correct predictions: 11728, its 66.636%\n",
      "Epoch:  103 |---> loss is 1.0764963627, total correct predictions: 11752, its 66.773%\n",
      "Epoch:  104 |---> loss is 1.0760968924, total correct predictions: 11772, its 66.886%\n",
      "Epoch:  105 |---> loss is 1.0751289129, total correct predictions: 11793, its 67.006%\n",
      "Epoch:  106 |---> loss is 1.0744092464, total correct predictions: 11822, its 67.170%\n",
      "Epoch:  107 |---> loss is 1.0741297007, total correct predictions: 11828, its 67.205%\n",
      "Epoch:  108 |---> loss is 1.0746327639, total correct predictions: 11791, its 66.994%\n",
      "Epoch:  109 |---> loss is 1.0835545063, total correct predictions: 11528, its 65.500%\n",
      "Epoch:  110 |---> loss is 1.1106262207, total correct predictions: 10986, its 62.420%\n",
      "Epoch:  111 |---> loss is 1.0825909376, total correct predictions: 11575, its 65.767%\n",
      "Epoch:  112 |---> loss is 1.1243702173, total correct predictions: 10749, its 61.074%\n",
      "Epoch:  113 |---> loss is 1.1195467710, total correct predictions: 10772, its 61.205%\n",
      "Epoch:  114 |---> loss is 1.0946750641, total correct predictions: 11310, its 64.261%\n",
      "Epoch:  115 |---> loss is 1.1070555449, total correct predictions: 11011, its 62.562%\n",
      "Epoch:  116 |---> loss is 1.0890591145, total correct predictions: 11419, its 64.881%\n",
      "Epoch:  117 |---> loss is 1.0899955034, total correct predictions: 11367, its 64.585%\n",
      "Epoch:  118 |---> loss is 1.0997627974, total correct predictions: 11144, its 63.318%\n",
      "Epoch:  119 |---> loss is 1.0749379396, total correct predictions: 11764, its 66.841%\n",
      "Epoch:  120 |---> loss is 1.0918911695, total correct predictions: 11360, its 64.545%\n",
      "Epoch:  121 |---> loss is 1.0883736610, total correct predictions: 11459, its 65.108%\n",
      "Epoch:  122 |---> loss is 1.0788121223, total correct predictions: 11658, its 66.239%\n",
      "Epoch:  123 |---> loss is 1.0864362717, total correct predictions: 11456, its 65.091%\n",
      "Epoch:  124 |---> loss is 1.0858608484, total correct predictions: 11472, its 65.182%\n",
      "Epoch:  125 |---> loss is 1.0754665136, total correct predictions: 11723, its 66.608%\n",
      "Epoch:  126 |---> loss is 1.0778232813, total correct predictions: 11678, its 66.352%\n",
      "Epoch:  127 |---> loss is 1.0794004202, total correct predictions: 11652, its 66.205%\n",
      "Epoch:  128 |---> loss is 1.0704081059, total correct predictions: 11825, its 67.188%\n",
      "Epoch:  129 |---> loss is 1.0775175095, total correct predictions: 11673, its 66.324%\n",
      "Epoch:  130 |---> loss is 1.0696902275, total correct predictions: 11842, its 67.284%\n",
      "Epoch:  131 |---> loss is 1.0734119415, total correct predictions: 11784, its 66.955%\n",
      "Epoch:  132 |---> loss is 1.0721539259, total correct predictions: 11788, its 66.977%\n",
      "Epoch:  133 |---> loss is 1.0673881769, total correct predictions: 11904, its 67.636%\n",
      "Epoch:  134 |---> loss is 1.0724688768, total correct predictions: 11762, its 66.830%\n",
      "Epoch:  135 |---> loss is 1.0665174723, total correct predictions: 11938, its 67.830%\n",
      "Epoch:  136 |---> loss is 1.0688756704, total correct predictions: 11878, its 67.489%\n",
      "Epoch:  137 |---> loss is 1.0657520294, total correct predictions: 11947, its 67.881%\n",
      "Epoch:  138 |---> loss is 1.0662698746, total correct predictions: 11927, its 67.767%\n",
      "Epoch:  139 |---> loss is 1.0659878254, total correct predictions: 11913, its 67.688%\n",
      "Epoch:  140 |---> loss is 1.0640093088, total correct predictions: 11991, its 68.131%\n",
      "Epoch:  141 |---> loss is 1.0650786161, total correct predictions: 11952, its 67.909%\n",
      "Epoch:  142 |---> loss is 1.0626968145, total correct predictions: 12011, its 68.244%\n",
      "Epoch:  143 |---> loss is 1.0655802488, total correct predictions: 11917, its 67.710%\n",
      "Epoch:  144 |---> loss is 1.0648305416, total correct predictions: 11961, its 67.960%\n",
      "Epoch:  145 |---> loss is 1.0727803707, total correct predictions: 11754, its 66.784%\n",
      "Epoch:  146 |---> loss is 1.0749193430, total correct predictions: 11705, its 66.506%\n",
      "Epoch:  147 |---> loss is 1.0676476955, total correct predictions: 11853, its 67.347%\n",
      "Epoch:  148 |---> loss is 1.0595924854, total correct predictions: 12079, its 68.631%\n",
      "Epoch:  149 |---> loss is 1.0606675148, total correct predictions: 12057, its 68.506%\n",
      "Epoch:  150 |---> loss is 1.0647460222, total correct predictions: 11948, its 67.886%\n",
      "Epoch:  151 |---> loss is 1.0711698532, total correct predictions: 11791, its 66.994%\n",
      "Epoch:  152 |---> loss is 1.0650433302, total correct predictions: 11926, its 67.761%\n",
      "Epoch:  153 |---> loss is 1.0574735403, total correct predictions: 12132, its 68.932%\n",
      "Epoch:  154 |---> loss is 1.0594025850, total correct predictions: 12087, its 68.676%\n",
      "Epoch:  155 |---> loss is 1.0631173849, total correct predictions: 12001, its 68.188%\n",
      "Epoch:  156 |---> loss is 1.0676083565, total correct predictions: 11882, its 67.511%\n",
      "Epoch:  157 |---> loss is 1.0579680204, total correct predictions: 12100, its 68.750%\n",
      "Epoch:  158 |---> loss is 1.0559825897, total correct predictions: 12142, its 68.989%\n",
      "Epoch:  159 |---> loss is 1.0582062006, total correct predictions: 12101, its 68.756%\n",
      "Epoch:  160 |---> loss is 1.0647668839, total correct predictions: 11936, its 67.818%\n",
      "Epoch:  161 |---> loss is 1.0623447895, total correct predictions: 12014, its 68.261%\n",
      "Epoch:  162 |---> loss is 1.0552755594, total correct predictions: 12140, its 68.977%\n",
      "Epoch:  163 |---> loss is 1.0526214838, total correct predictions: 12210, its 69.375%\n",
      "Epoch:  164 |---> loss is 1.0562338829, total correct predictions: 12152, its 69.045%\n",
      "Epoch:  165 |---> loss is 1.0585100651, total correct predictions: 12078, its 68.625%\n",
      "Epoch:  166 |---> loss is 1.0592778921, total correct predictions: 12098, its 68.739%\n",
      "Epoch:  167 |---> loss is 1.0546945333, total correct predictions: 12162, its 69.102%\n",
      "Epoch:  168 |---> loss is 1.0510295630, total correct predictions: 12245, its 69.574%\n",
      "Epoch:  169 |---> loss is 1.0501379967, total correct predictions: 12261, its 69.665%\n",
      "Epoch:  170 |---> loss is 1.0521587133, total correct predictions: 12220, its 69.432%\n",
      "Epoch:  171 |---> loss is 1.0581661463, total correct predictions: 12109, its 68.801%\n",
      "Epoch:  172 |---> loss is 1.0607583523, total correct predictions: 12010, its 68.239%\n",
      "Epoch:  173 |---> loss is 1.0580768585, total correct predictions: 12103, its 68.767%\n",
      "Epoch:  174 |---> loss is 1.0514513254, total correct predictions: 12228, its 69.477%\n",
      "Epoch:  175 |---> loss is 1.0484082699, total correct predictions: 12310, its 69.943%\n",
      "Epoch:  176 |---> loss is 1.0474524498, total correct predictions: 12328, its 70.045%\n",
      "Epoch:  177 |---> loss is 1.0493465662, total correct predictions: 12276, its 69.750%\n",
      "Epoch:  178 |---> loss is 1.0529941320, total correct predictions: 12209, its 69.369%\n",
      "Epoch:  179 |---> loss is 1.0559747219, total correct predictions: 12115, its 68.835%\n",
      "Epoch:  180 |---> loss is 1.0563797951, total correct predictions: 12126, its 68.898%\n",
      "Epoch:  181 |---> loss is 1.0491756201, total correct predictions: 12277, its 69.756%\n",
      "Epoch:  182 |---> loss is 1.0464950800, total correct predictions: 12348, its 70.159%\n",
      "Epoch:  183 |---> loss is 1.0440753698, total correct predictions: 12393, its 70.415%\n",
      "Epoch:  184 |---> loss is 1.0466830730, total correct predictions: 12324, its 70.023%\n",
      "Epoch:  185 |---> loss is 1.0508987904, total correct predictions: 12248, its 69.591%\n",
      "Epoch:  186 |---> loss is 1.0556490421, total correct predictions: 12104, its 68.773%\n",
      "Epoch:  187 |---> loss is 1.0572365522, total correct predictions: 12093, its 68.710%\n",
      "Epoch:  188 |---> loss is 1.0451076031, total correct predictions: 12357, its 70.210%\n",
      "Epoch:  189 |---> loss is 1.0444320440, total correct predictions: 12371, its 70.290%\n",
      "Epoch:  190 |---> loss is 1.0477479696, total correct predictions: 12301, its 69.892%\n",
      "Epoch:  191 |---> loss is 1.0546534061, total correct predictions: 12124, its 68.886%\n",
      "Epoch:  192 |---> loss is 1.0606312752, total correct predictions: 11991, its 68.131%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  193 |---> loss is 1.0454518795, total correct predictions: 12344, its 70.136%\n",
      "Epoch:  194 |---> loss is 1.0453662872, total correct predictions: 12335, its 70.085%\n",
      "Epoch:  195 |---> loss is 1.0457941294, total correct predictions: 12335, its 70.085%\n",
      "Epoch:  196 |---> loss is 1.0505398512, total correct predictions: 12223, its 69.449%\n",
      "Epoch:  197 |---> loss is 1.0475929976, total correct predictions: 12269, its 69.710%\n",
      "Epoch:  198 |---> loss is 1.0400625467, total correct predictions: 12449, its 70.733%\n",
      "Epoch:  199 |---> loss is 1.0513956547, total correct predictions: 12187, its 69.244%\n",
      "Epoch:  200 |---> loss is 1.0464189053, total correct predictions: 12324, its 70.023%\n",
      "Epoch:  201 |---> loss is 1.0461874008, total correct predictions: 12303, its 69.903%\n",
      "Epoch:  202 |---> loss is 1.0428760052, total correct predictions: 12366, its 70.261%\n",
      "Epoch:  203 |---> loss is 1.0386688709, total correct predictions: 12471, its 70.858%\n",
      "Epoch:  204 |---> loss is 1.0461387634, total correct predictions: 12301, its 69.892%\n",
      "Epoch:  205 |---> loss is 1.0423151255, total correct predictions: 12392, its 70.409%\n",
      "Epoch:  206 |---> loss is 1.0455160141, total correct predictions: 12327, its 70.040%\n",
      "Epoch:  207 |---> loss is 1.0408850908, total correct predictions: 12430, its 70.625%\n",
      "Epoch:  208 |---> loss is 1.0364630222, total correct predictions: 12523, its 71.153%\n",
      "Epoch:  209 |---> loss is 1.0400673151, total correct predictions: 12434, its 70.648%\n",
      "Epoch:  210 |---> loss is 1.0380523205, total correct predictions: 12496, its 71.000%\n",
      "Epoch:  211 |---> loss is 1.0437303782, total correct predictions: 12339, its 70.108%\n",
      "Epoch:  212 |---> loss is 1.0490105152, total correct predictions: 12227, its 69.472%\n",
      "Epoch:  213 |---> loss is 1.0500402451, total correct predictions: 12223, its 69.449%\n",
      "Epoch:  214 |---> loss is 1.0423579216, total correct predictions: 12381, its 70.347%\n",
      "Epoch:  215 |---> loss is 1.0332351923, total correct predictions: 12598, its 71.580%\n",
      "Epoch:  216 |---> loss is 1.0365908146, total correct predictions: 12492, its 70.977%\n",
      "Epoch:  217 |---> loss is 1.0419880152, total correct predictions: 12402, its 70.466%\n",
      "Epoch:  218 |---> loss is 1.0462526083, total correct predictions: 12288, its 69.818%\n",
      "Epoch:  219 |---> loss is 1.0441597700, total correct predictions: 12353, its 70.188%\n",
      "Epoch:  220 |---> loss is 1.0347069502, total correct predictions: 12543, its 71.267%\n",
      "Epoch:  221 |---> loss is 1.0340822935, total correct predictions: 12551, its 71.312%\n",
      "Epoch:  222 |---> loss is 1.0339244604, total correct predictions: 12571, its 71.426%\n",
      "Epoch:  223 |---> loss is 1.0368663073, total correct predictions: 12501, its 71.028%\n",
      "Epoch:  224 |---> loss is 1.0410293341, total correct predictions: 12402, its 70.466%\n",
      "Epoch:  225 |---> loss is 1.0315169096, total correct predictions: 12599, its 71.585%\n",
      "Epoch:  226 |---> loss is 1.0323240757, total correct predictions: 12600, its 71.591%\n",
      "Epoch:  227 |---> loss is 1.0336420536, total correct predictions: 12568, its 71.409%\n",
      "Epoch:  228 |---> loss is 1.0349611044, total correct predictions: 12555, its 71.335%\n",
      "Epoch:  229 |---> loss is 1.0427496433, total correct predictions: 12361, its 70.233%\n",
      "Epoch:  230 |---> loss is 1.0376782417, total correct predictions: 12488, its 70.955%\n",
      "Epoch:  231 |---> loss is 1.0401374102, total correct predictions: 12413, its 70.528%\n",
      "Epoch:  232 |---> loss is 1.0295367241, total correct predictions: 12653, its 71.892%\n",
      "Epoch:  233 |---> loss is 1.0275743008, total correct predictions: 12681, its 72.051%\n",
      "Epoch:  234 |---> loss is 1.0317230225, total correct predictions: 12596, its 71.568%\n",
      "Epoch:  235 |---> loss is 1.0333652496, total correct predictions: 12593, its 71.551%\n",
      "Epoch:  236 |---> loss is 1.0386955738, total correct predictions: 12457, its 70.778%\n",
      "Epoch:  237 |---> loss is 1.0372138023, total correct predictions: 12505, its 71.051%\n",
      "Epoch:  238 |---> loss is 1.0367162228, total correct predictions: 12481, its 70.915%\n",
      "Epoch:  239 |---> loss is 1.0287069082, total correct predictions: 12676, its 72.023%\n",
      "Epoch:  240 |---> loss is 1.0260267258, total correct predictions: 12701, its 72.165%\n",
      "Epoch:  241 |---> loss is 1.0264575481, total correct predictions: 12701, its 72.165%\n",
      "Epoch:  242 |---> loss is 1.0285234451, total correct predictions: 12682, its 72.057%\n",
      "Epoch:  243 |---> loss is 1.0332561731, total correct predictions: 12559, its 71.358%\n",
      "Epoch:  244 |---> loss is 1.0265060663, total correct predictions: 12712, its 72.227%\n",
      "Epoch:  245 |---> loss is 1.0239069462, total correct predictions: 12734, its 72.352%\n",
      "Epoch:  246 |---> loss is 1.0235992670, total correct predictions: 12748, its 72.432%\n",
      "Epoch:  247 |---> loss is 1.0241564512, total correct predictions: 12765, its 72.528%\n",
      "Epoch:  248 |---> loss is 1.0276414156, total correct predictions: 12666, its 71.966%\n",
      "Epoch:  249 |---> loss is 1.0313355923, total correct predictions: 12633, its 71.778%\n",
      "Epoch:  250 |---> loss is 1.0455006361, total correct predictions: 12267, its 69.699%\n",
      "Epoch:  251 |---> loss is 1.0416934490, total correct predictions: 12366, its 70.261%\n",
      "Epoch:  252 |---> loss is 1.0391649008, total correct predictions: 12427, its 70.608%\n",
      "Epoch:  253 |---> loss is 1.0257861614, total correct predictions: 12736, its 72.364%\n",
      "Epoch:  254 |---> loss is 1.0217361450, total correct predictions: 12779, its 72.608%\n",
      "Epoch:  255 |---> loss is 1.0252884626, total correct predictions: 12714, its 72.239%\n",
      "Epoch:  256 |---> loss is 1.0262157917, total correct predictions: 12736, its 72.364%\n",
      "Epoch:  257 |---> loss is 1.0279309750, total correct predictions: 12643, its 71.835%\n",
      "Epoch:  258 |---> loss is 1.0231888294, total correct predictions: 12759, its 72.494%\n",
      "Epoch:  259 |---> loss is 1.0200754404, total correct predictions: 12801, its 72.733%\n",
      "Epoch:  260 |---> loss is 1.0201613903, total correct predictions: 12792, its 72.682%\n",
      "Epoch:  261 |---> loss is 1.0225259066, total correct predictions: 12777, its 72.597%\n",
      "Epoch:  262 |---> loss is 1.0257011652, total correct predictions: 12685, its 72.074%\n",
      "Epoch:  263 |---> loss is 1.0229247808, total correct predictions: 12775, its 72.585%\n",
      "Epoch:  264 |---> loss is 1.0237849951, total correct predictions: 12735, its 72.358%\n",
      "Epoch:  265 |---> loss is 1.0217603445, total correct predictions: 12801, its 72.733%\n",
      "Epoch:  266 |---> loss is 1.0200412273, total correct predictions: 12812, its 72.795%\n",
      "Epoch:  267 |---> loss is 1.0176062584, total correct predictions: 12866, its 73.102%\n",
      "Epoch:  268 |---> loss is 1.0170793533, total correct predictions: 12858, its 73.057%\n",
      "Epoch:  269 |---> loss is 1.0163859129, total correct predictions: 12888, its 73.227%\n",
      "Epoch:  270 |---> loss is 1.0156209469, total correct predictions: 12901, its 73.301%\n",
      "Epoch:  271 |---> loss is 1.0158929825, total correct predictions: 12890, its 73.239%\n",
      "Epoch:  272 |---> loss is 1.0185537338, total correct predictions: 12858, its 73.057%\n",
      "Epoch:  273 |---> loss is 1.0351059437, total correct predictions: 12473, its 70.869%\n",
      "Epoch:  274 |---> loss is 1.0373766422, total correct predictions: 12419, its 70.562%\n",
      "Epoch:  275 |---> loss is 1.0402846336, total correct predictions: 12366, its 70.261%\n",
      "Epoch:  276 |---> loss is 1.0322186947, total correct predictions: 12541, its 71.256%\n",
      "Epoch:  277 |---> loss is 1.0266715288, total correct predictions: 12670, its 71.989%\n",
      "Epoch:  278 |---> loss is 1.0174461603, total correct predictions: 12865, its 73.097%\n",
      "Epoch:  279 |---> loss is 1.0146436691, total correct predictions: 12915, its 73.381%\n",
      "Epoch:  280 |---> loss is 1.0191192627, total correct predictions: 12832, its 72.909%\n",
      "Epoch:  281 |---> loss is 1.0214748383, total correct predictions: 12788, its 72.659%\n",
      "Epoch:  282 |---> loss is 1.0171990395, total correct predictions: 12849, its 73.006%\n",
      "Epoch:  283 |---> loss is 1.0261991024, total correct predictions: 12668, its 71.977%\n",
      "Epoch:  284 |---> loss is 1.0502005816, total correct predictions: 12157, its 69.074%\n",
      "Epoch:  285 |---> loss is 1.0245931149, total correct predictions: 12687, its 72.085%\n",
      "Epoch:  286 |---> loss is 1.0789557695, total correct predictions: 11679, its 66.358%\n",
      "Epoch:  287 |---> loss is 1.0873838663, total correct predictions: 11519, its 65.449%\n",
      "Epoch:  288 |---> loss is 1.0520880222, total correct predictions: 12133, its 68.938%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  289 |---> loss is 1.0814913511, total correct predictions: 11644, its 66.159%\n",
      "Epoch:  290 |---> loss is 1.1120885611, total correct predictions: 11043, its 62.744%\n",
      "Epoch:  291 |---> loss is 1.1091033220, total correct predictions: 11147, its 63.335%\n",
      "Epoch:  292 |---> loss is 1.1050490141, total correct predictions: 11206, its 63.670%\n",
      "Epoch:  293 |---> loss is 1.0975196362, total correct predictions: 11338, its 64.420%\n",
      "Epoch:  294 |---> loss is 1.0776634216, total correct predictions: 11675, its 66.335%\n",
      "Epoch:  295 |---> loss is 1.0451265574, total correct predictions: 12307, its 69.926%\n",
      "Epoch:  296 |---> loss is 1.0884183645, total correct predictions: 11464, its 65.136%\n",
      "Epoch:  297 |---> loss is 1.0939145088, total correct predictions: 11366, its 64.580%\n",
      "Epoch:  298 |---> loss is 1.0503104925, total correct predictions: 12174, its 69.170%\n",
      "Epoch:  299 |---> loss is 1.0475200415, total correct predictions: 12241, its 69.551%\n",
      "Epoch:  300 |---> loss is 1.0704381466, total correct predictions: 11801, its 67.051%\n",
      "Epoch:  301 |---> loss is 1.0649887323, total correct predictions: 11907, its 67.653%\n",
      "Epoch:  302 |---> loss is 1.0413616896, total correct predictions: 12356, its 70.205%\n",
      "Epoch:  303 |---> loss is 1.0509133339, total correct predictions: 12187, its 69.244%\n",
      "Epoch:  304 |---> loss is 1.0584146976, total correct predictions: 12029, its 68.347%\n",
      "Epoch:  305 |---> loss is 1.0379301310, total correct predictions: 12415, its 70.540%\n",
      "Epoch:  306 |---> loss is 1.0493133068, total correct predictions: 12208, its 69.364%\n",
      "Epoch:  307 |---> loss is 1.0487083197, total correct predictions: 12233, its 69.506%\n",
      "Epoch:  308 |---> loss is 1.0325500965, total correct predictions: 12539, its 71.244%\n",
      "Epoch:  309 |---> loss is 1.0441722870, total correct predictions: 12299, its 69.881%\n",
      "Epoch:  310 |---> loss is 1.0406466722, total correct predictions: 12363, its 70.244%\n",
      "Epoch:  311 |---> loss is 1.0302208662, total correct predictions: 12588, its 71.523%\n",
      "Epoch:  312 |---> loss is 1.0380488634, total correct predictions: 12425, its 70.597%\n",
      "Epoch:  313 |---> loss is 1.0313194990, total correct predictions: 12567, its 71.403%\n",
      "Epoch:  314 |---> loss is 1.0257889032, total correct predictions: 12665, its 71.960%\n",
      "Epoch:  315 |---> loss is 1.0328618288, total correct predictions: 12541, its 71.256%\n",
      "Epoch:  316 |---> loss is 1.0226743221, total correct predictions: 12720, its 72.273%\n",
      "Epoch:  317 |---> loss is 1.0280623436, total correct predictions: 12619, its 71.699%\n",
      "Epoch:  318 |---> loss is 1.0243163109, total correct predictions: 12706, its 72.193%\n",
      "Epoch:  319 |---> loss is 1.0218863487, total correct predictions: 12740, its 72.386%\n",
      "Epoch:  320 |---> loss is 1.0237822533, total correct predictions: 12730, its 72.330%\n",
      "Epoch:  321 |---> loss is 1.0182546377, total correct predictions: 12821, its 72.847%\n",
      "Epoch:  322 |---> loss is 1.0220371485, total correct predictions: 12733, its 72.347%\n",
      "Epoch:  323 |---> loss is 1.0166478157, total correct predictions: 12847, its 72.994%\n",
      "Epoch:  324 |---> loss is 1.0189217329, total correct predictions: 12806, its 72.761%\n",
      "Epoch:  325 |---> loss is 1.0157402754, total correct predictions: 12871, its 73.131%\n",
      "Epoch:  326 |---> loss is 1.0155998468, total correct predictions: 12891, its 73.244%\n",
      "Epoch:  327 |---> loss is 1.0148215294, total correct predictions: 12909, its 73.347%\n",
      "Epoch:  328 |---> loss is 1.0133856535, total correct predictions: 12914, its 73.375%\n",
      "Epoch:  329 |---> loss is 1.0138509274, total correct predictions: 12913, its 73.369%\n",
      "Epoch:  330 |---> loss is 1.0116730928, total correct predictions: 12966, its 73.670%\n",
      "Epoch:  331 |---> loss is 1.0126346350, total correct predictions: 12949, its 73.574%\n",
      "Epoch:  332 |---> loss is 1.0105214119, total correct predictions: 12998, its 73.852%\n",
      "Epoch:  333 |---> loss is 1.0119168758, total correct predictions: 12960, its 73.636%\n",
      "Epoch:  334 |---> loss is 1.0119096041, total correct predictions: 12957, its 73.619%\n",
      "Epoch:  335 |---> loss is 1.0212380886, total correct predictions: 12748, its 72.432%\n",
      "Epoch:  336 |---> loss is 1.0393331051, total correct predictions: 12362, its 70.239%\n",
      "Epoch:  337 |---> loss is 1.0348589420, total correct predictions: 12444, its 70.705%\n",
      "Epoch:  338 |---> loss is 1.0249629021, total correct predictions: 12674, its 72.011%\n",
      "Epoch:  339 |---> loss is 1.0186041594, total correct predictions: 12815, its 72.812%\n",
      "Epoch:  340 |---> loss is 1.0152075291, total correct predictions: 12863, its 73.085%\n",
      "Epoch:  341 |---> loss is 1.0091142654, total correct predictions: 12989, its 73.801%\n",
      "Epoch:  342 |---> loss is 1.0102901459, total correct predictions: 12971, its 73.699%\n",
      "Epoch:  343 |---> loss is 1.0145834684, total correct predictions: 12871, its 73.131%\n",
      "Epoch:  344 |---> loss is 1.0141948462, total correct predictions: 12914, its 73.375%\n",
      "Epoch:  345 |---> loss is 1.0141799450, total correct predictions: 12905, its 73.324%\n",
      "Epoch:  346 |---> loss is 1.0104137659, total correct predictions: 12976, its 73.727%\n",
      "Epoch:  347 |---> loss is 1.0070806742, total correct predictions: 13026, its 74.011%\n",
      "Epoch:  348 |---> loss is 1.0069712400, total correct predictions: 13027, its 74.017%\n",
      "Epoch:  349 |---> loss is 1.0065478086, total correct predictions: 13038, its 74.080%\n",
      "Epoch:  350 |---> loss is 1.0077027082, total correct predictions: 13017, its 73.960%\n",
      "Epoch:  351 |---> loss is 1.0104947090, total correct predictions: 12982, its 73.761%\n",
      "Epoch:  352 |---> loss is 1.0149190426, total correct predictions: 12854, its 73.034%\n",
      "Epoch:  353 |---> loss is 1.0191842318, total correct predictions: 12807, its 72.767%\n",
      "Epoch:  354 |---> loss is 1.0289852619, total correct predictions: 12590, its 71.534%\n",
      "Epoch:  355 |---> loss is 1.0203046799, total correct predictions: 12789, its 72.665%\n",
      "Epoch:  356 |---> loss is 1.0123426914, total correct predictions: 12935, its 73.494%\n",
      "Epoch:  357 |---> loss is 1.0048570633, total correct predictions: 13065, its 74.233%\n",
      "Epoch:  358 |---> loss is 1.0054378510, total correct predictions: 13049, its 74.142%\n",
      "Epoch:  359 |---> loss is 1.0095255375, total correct predictions: 12976, its 73.727%\n",
      "Epoch:  360 |---> loss is 1.0078834295, total correct predictions: 13035, its 74.062%\n",
      "Epoch:  361 |---> loss is 1.0039254427, total correct predictions: 13069, its 74.256%\n",
      "Epoch:  362 |---> loss is 1.0032019615, total correct predictions: 13088, its 74.364%\n",
      "Epoch:  363 |---> loss is 1.0055726767, total correct predictions: 13076, its 74.295%\n",
      "Epoch:  364 |---> loss is 1.0055716038, total correct predictions: 13052, its 74.159%\n",
      "Epoch:  365 |---> loss is 1.0023201704, total correct predictions: 13107, its 74.472%\n",
      "Epoch:  366 |---> loss is 1.0023853779, total correct predictions: 13111, its 74.494%\n",
      "Epoch:  367 |---> loss is 1.0049173832, total correct predictions: 13074, its 74.284%\n",
      "Epoch:  368 |---> loss is 1.0049458742, total correct predictions: 13091, its 74.381%\n",
      "Epoch:  369 |---> loss is 1.0037589073, total correct predictions: 13101, its 74.438%\n",
      "Epoch:  370 |---> loss is 1.0009428263, total correct predictions: 13133, its 74.619%\n",
      "Epoch:  371 |---> loss is 1.0002682209, total correct predictions: 13149, its 74.710%\n",
      "Epoch:  372 |---> loss is 1.0021393299, total correct predictions: 13126, its 74.580%\n",
      "Epoch:  373 |---> loss is 1.0025097132, total correct predictions: 13132, its 74.614%\n",
      "Epoch:  374 |---> loss is 1.0030437708, total correct predictions: 13114, its 74.511%\n",
      "Epoch:  375 |---> loss is 1.0041482449, total correct predictions: 13088, its 74.364%\n",
      "Epoch:  376 |---> loss is 1.0097999573, total correct predictions: 12953, its 73.597%\n",
      "Epoch:  377 |---> loss is 1.0126529932, total correct predictions: 12887, its 73.222%\n",
      "Epoch:  378 |---> loss is 1.0214792490, total correct predictions: 12717, its 72.256%\n",
      "Epoch:  379 |---> loss is 1.0203020573, total correct predictions: 12762, its 72.511%\n",
      "Epoch:  380 |---> loss is 1.0167595148, total correct predictions: 12827, its 72.881%\n",
      "Epoch:  381 |---> loss is 1.0070633888, total correct predictions: 13031, its 74.040%\n",
      "Epoch:  382 |---> loss is 0.9998571873, total correct predictions: 13151, its 74.722%\n",
      "Epoch:  383 |---> loss is 1.0013161898, total correct predictions: 13137, its 74.642%\n",
      "Epoch:  384 |---> loss is 1.0042244196, total correct predictions: 13088, its 74.364%\n",
      "Epoch:  385 |---> loss is 1.0015974045, total correct predictions: 13122, its 74.557%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  386 |---> loss is 0.9983530641, total correct predictions: 13171, its 74.835%\n",
      "Epoch:  387 |---> loss is 1.0007119179, total correct predictions: 13144, its 74.682%\n",
      "Epoch:  388 |---> loss is 0.9991465211, total correct predictions: 13168, its 74.818%\n",
      "Epoch:  389 |---> loss is 0.9986441135, total correct predictions: 13175, its 74.858%\n",
      "Epoch:  390 |---> loss is 0.9997013807, total correct predictions: 13160, its 74.773%\n",
      "Epoch:  391 |---> loss is 0.9979364276, total correct predictions: 13182, its 74.898%\n",
      "Epoch:  392 |---> loss is 0.9976766706, total correct predictions: 13194, its 74.966%\n",
      "Epoch:  393 |---> loss is 0.9972169995, total correct predictions: 13202, its 75.011%\n",
      "Epoch:  394 |---> loss is 0.9959838986, total correct predictions: 13216, its 75.091%\n",
      "Epoch:  395 |---> loss is 0.9968955517, total correct predictions: 13210, its 75.057%\n",
      "Epoch:  396 |---> loss is 0.9961053729, total correct predictions: 13224, its 75.136%\n",
      "Epoch:  397 |---> loss is 0.9967851043, total correct predictions: 13220, its 75.114%\n",
      "Epoch:  398 |---> loss is 0.9977983832, total correct predictions: 13203, its 75.017%\n",
      "Epoch:  399 |---> loss is 1.0022205114, total correct predictions: 13104, its 74.455%\n",
      "Epoch:  400 |---> loss is 1.0178070068, total correct predictions: 12755, its 72.472%\n",
      "Epoch:  401 |---> loss is 1.0302807093, total correct predictions: 12513, its 71.097%\n",
      "Epoch:  402 |---> loss is 1.0010838509, total correct predictions: 13139, its 74.653%\n",
      "Epoch:  403 |---> loss is 1.0146160126, total correct predictions: 12847, its 72.994%\n",
      "Epoch:  404 |---> loss is 1.0351866484, total correct predictions: 12432, its 70.636%\n",
      "Epoch:  405 |---> loss is 1.0170109272, total correct predictions: 12816, its 72.818%\n",
      "Epoch:  406 |---> loss is 1.0586776733, total correct predictions: 11940, its 67.841%\n",
      "Epoch:  407 |---> loss is 1.0234748125, total correct predictions: 12666, its 71.966%\n",
      "Epoch:  408 |---> loss is 1.0673742294, total correct predictions: 11789, its 66.983%\n",
      "Epoch:  409 |---> loss is 1.0734670162, total correct predictions: 11697, its 66.460%\n",
      "Epoch:  410 |---> loss is 1.0479635000, total correct predictions: 12177, its 69.188%\n",
      "Epoch:  411 |---> loss is 1.0490313768, total correct predictions: 12162, its 69.102%\n",
      "Epoch:  412 |---> loss is 1.0491453409, total correct predictions: 12178, its 69.193%\n",
      "Epoch:  413 |---> loss is 1.0229402781, total correct predictions: 12676, its 72.023%\n",
      "Epoch:  414 |---> loss is 1.0383095741, total correct predictions: 12380, its 70.341%\n",
      "Epoch:  415 |---> loss is 1.0314350128, total correct predictions: 12511, its 71.085%\n",
      "Epoch:  416 |---> loss is 1.0146553516, total correct predictions: 12844, its 72.977%\n",
      "Epoch:  417 |---> loss is 1.0350252390, total correct predictions: 12428, its 70.614%\n",
      "Epoch:  418 |---> loss is 1.0184172392, total correct predictions: 12768, its 72.545%\n",
      "Epoch:  419 |---> loss is 1.0187653303, total correct predictions: 12766, its 72.534%\n",
      "Epoch:  420 |---> loss is 1.0259414911, total correct predictions: 12624, its 71.727%\n",
      "Epoch:  421 |---> loss is 1.0069756508, total correct predictions: 12991, its 73.812%\n",
      "Epoch:  422 |---> loss is 1.0199637413, total correct predictions: 12745, its 72.415%\n",
      "Epoch:  423 |---> loss is 1.0098445415, total correct predictions: 12948, its 73.568%\n",
      "Epoch:  424 |---> loss is 1.0101807117, total correct predictions: 12928, its 73.455%\n",
      "Epoch:  425 |---> loss is 1.0134358406, total correct predictions: 12878, its 73.170%\n",
      "Epoch:  426 |---> loss is 1.0019415617, total correct predictions: 13087, its 74.358%\n",
      "Epoch:  427 |---> loss is 1.0142220259, total correct predictions: 12845, its 72.983%\n",
      "Epoch:  428 |---> loss is 1.0005931854, total correct predictions: 13113, its 74.506%\n",
      "Epoch:  429 |---> loss is 1.0079349279, total correct predictions: 12968, its 73.682%\n",
      "Epoch:  430 |---> loss is 1.0057667494, total correct predictions: 13041, its 74.097%\n",
      "Epoch:  431 |---> loss is 1.0000175238, total correct predictions: 13140, its 74.659%\n",
      "Epoch:  432 |---> loss is 1.0072063208, total correct predictions: 13023, its 73.994%\n",
      "Epoch:  433 |---> loss is 0.9993533492, total correct predictions: 13135, its 74.631%\n",
      "Epoch:  434 |---> loss is 1.0064737797, total correct predictions: 13012, its 73.932%\n",
      "Epoch:  435 |---> loss is 0.9957613945, total correct predictions: 13198, its 74.989%\n",
      "Epoch:  436 |---> loss is 1.0049825907, total correct predictions: 13019, its 73.972%\n",
      "Epoch:  437 |---> loss is 1.0020576715, total correct predictions: 13090, its 74.375%\n",
      "Epoch:  438 |---> loss is 1.0063425303, total correct predictions: 13019, its 73.972%\n",
      "Epoch:  439 |---> loss is 0.9994944334, total correct predictions: 13140, its 74.659%\n",
      "Epoch:  440 |---> loss is 0.9978362322, total correct predictions: 13185, its 74.915%\n",
      "Epoch:  441 |---> loss is 0.9952580333, total correct predictions: 13222, its 75.125%\n",
      "Epoch:  442 |---> loss is 0.9982418418, total correct predictions: 13167, its 74.812%\n",
      "Epoch:  443 |---> loss is 0.9956597090, total correct predictions: 13224, its 75.136%\n",
      "Epoch:  444 |---> loss is 0.9929509759, total correct predictions: 13260, its 75.341%\n",
      "Epoch:  445 |---> loss is 0.9953234792, total correct predictions: 13234, its 75.193%\n",
      "Epoch:  446 |---> loss is 0.9959016442, total correct predictions: 13225, its 75.142%\n",
      "Epoch:  447 |---> loss is 0.9954071641, total correct predictions: 13215, its 75.085%\n",
      "Epoch:  448 |---> loss is 0.9915369749, total correct predictions: 13286, its 75.489%\n",
      "Epoch:  449 |---> loss is 0.9925820231, total correct predictions: 13271, its 75.403%\n",
      "Epoch:  450 |---> loss is 0.9958888888, total correct predictions: 13221, its 75.119%\n",
      "Epoch:  451 |---> loss is 0.9966869950, total correct predictions: 13221, its 75.119%\n",
      "Epoch:  452 |---> loss is 0.9947462082, total correct predictions: 13251, its 75.290%\n",
      "Epoch:  453 |---> loss is 0.9902682304, total correct predictions: 13309, its 75.619%\n",
      "Epoch:  454 |---> loss is 0.9905422330, total correct predictions: 13307, its 75.608%\n",
      "Epoch:  455 |---> loss is 0.9919826388, total correct predictions: 13280, its 75.455%\n",
      "Epoch:  456 |---> loss is 0.9904564023, total correct predictions: 13308, its 75.614%\n",
      "Epoch:  457 |---> loss is 0.9886252880, total correct predictions: 13331, its 75.744%\n",
      "Epoch:  458 |---> loss is 0.9895315766, total correct predictions: 13316, its 75.659%\n",
      "Epoch:  459 |---> loss is 0.9894739985, total correct predictions: 13332, its 75.750%\n",
      "Epoch:  460 |---> loss is 0.9879638553, total correct predictions: 13339, its 75.790%\n",
      "Epoch:  461 |---> loss is 0.9886215925, total correct predictions: 13335, its 75.767%\n",
      "Epoch:  462 |---> loss is 0.9881148934, total correct predictions: 13348, its 75.841%\n",
      "Epoch:  463 |---> loss is 0.9876859784, total correct predictions: 13351, its 75.858%\n",
      "Epoch:  464 |---> loss is 0.9872089028, total correct predictions: 13355, its 75.881%\n",
      "Epoch:  465 |---> loss is 0.9879974723, total correct predictions: 13350, its 75.852%\n",
      "Epoch:  466 |---> loss is 0.9869675636, total correct predictions: 13365, its 75.938%\n",
      "Epoch:  467 |---> loss is 0.9865804911, total correct predictions: 13364, its 75.932%\n",
      "Epoch:  468 |---> loss is 0.9864526391, total correct predictions: 13366, its 75.943%\n",
      "Epoch:  469 |---> loss is 0.9866040945, total correct predictions: 13371, its 75.972%\n",
      "Epoch:  470 |---> loss is 0.9861451387, total correct predictions: 13371, its 75.972%\n",
      "Epoch:  471 |---> loss is 0.9856290221, total correct predictions: 13378, its 76.011%\n",
      "Epoch:  472 |---> loss is 0.9858543873, total correct predictions: 13380, its 76.023%\n",
      "Epoch:  473 |---> loss is 0.9856811762, total correct predictions: 13381, its 76.028%\n",
      "Epoch:  474 |---> loss is 0.9855578542, total correct predictions: 13381, its 76.028%\n",
      "Epoch:  475 |---> loss is 0.9849772453, total correct predictions: 13386, its 76.057%\n",
      "Epoch:  476 |---> loss is 0.9850725532, total correct predictions: 13390, its 76.080%\n",
      "Epoch:  477 |---> loss is 0.9849882126, total correct predictions: 13389, its 76.074%\n",
      "Epoch:  478 |---> loss is 0.9849934578, total correct predictions: 13390, its 76.080%\n",
      "Epoch:  479 |---> loss is 0.9846666455, total correct predictions: 13392, its 76.091%\n",
      "Epoch:  480 |---> loss is 0.9844564795, total correct predictions: 13394, its 76.102%\n",
      "Epoch:  481 |---> loss is 0.9843018651, total correct predictions: 13396, its 76.114%\n",
      "Epoch:  482 |---> loss is 0.9843403697, total correct predictions: 13395, its 76.108%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  483 |---> loss is 0.9842269421, total correct predictions: 13397, its 76.119%\n",
      "Epoch:  484 |---> loss is 0.9840294123, total correct predictions: 13399, its 76.131%\n",
      "Epoch:  485 |---> loss is 0.9838637710, total correct predictions: 13400, its 76.136%\n",
      "Epoch:  486 |---> loss is 0.9837927222, total correct predictions: 13401, its 76.142%\n",
      "Epoch:  487 |---> loss is 0.9837772846, total correct predictions: 13405, its 76.165%\n",
      "Epoch:  488 |---> loss is 0.9836333394, total correct predictions: 13404, its 76.159%\n",
      "Epoch:  489 |---> loss is 0.9834952950, total correct predictions: 13408, its 76.182%\n",
      "Epoch:  490 |---> loss is 0.9833512902, total correct predictions: 13408, its 76.182%\n",
      "Epoch:  491 |---> loss is 0.9833208919, total correct predictions: 13408, its 76.182%\n",
      "Epoch:  492 |---> loss is 0.9832539558, total correct predictions: 13410, its 76.193%\n",
      "Epoch:  493 |---> loss is 0.9832044840, total correct predictions: 13411, its 76.199%\n",
      "Epoch:  494 |---> loss is 0.9830720425, total correct predictions: 13411, its 76.199%\n",
      "Epoch:  495 |---> loss is 0.9829860330, total correct predictions: 13412, its 76.205%\n",
      "Epoch:  496 |---> loss is 0.9828991890, total correct predictions: 13412, its 76.205%\n",
      "Epoch:  497 |---> loss is 0.9828795195, total correct predictions: 13412, its 76.205%\n",
      "Epoch:  498 |---> loss is 0.9828096032, total correct predictions: 13412, its 76.205%\n",
      "Epoch:  499 |---> loss is 0.9827459455, total correct predictions: 13412, its 76.205%\n"
     ]
    }
   ],
   "source": [
    "net_c = NetworkConv()\n",
    "print(net_c.conv1.weight)\n",
    "optimizer_c = optim.Adam(net_c.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "number_of_epoches_c = 500\n",
    "# Here define weights for loss contribution of segments\n",
    "loss_weights_c = torch.tensor([1.,1.,1.,1.])\n",
    "\n",
    "total_loss_c = []\n",
    "total_accuracy_c = []\n",
    "total_val_loss_c = []\n",
    "total_val_accuracy_c = []\n",
    "\n",
    "# Learning process\n",
    "for epoch in range(number_of_epoches_c):\n",
    "    predicted_c = net_c(train_features_c)\n",
    "    loss_c = F.cross_entropy(predicted_c, train_labels_c, loss_weights_c)\n",
    "    optimizer_c.zero_grad()\n",
    "    loss_c.backward()\n",
    "    optimizer_c.step()\n",
    "    \n",
    "    total_correct_c = get_correct_predictions(predicted_c, train_labels_c)\n",
    "    print(\"Epoch: {:4d} |---> loss is {:4.10f}, total correct predictions: {:5d}, its {:.3f}%\"\n",
    "      .format(epoch, loss_c.item(), total_correct_c, total_correct_c*100/train_features_c.shape[0]))\n",
    "    \n",
    "    with torch.no_grad():  # record loss and accuracy info for plots\n",
    "        total_loss_c.append(loss_c.item())\n",
    "        total_accuracy_c.append(total_correct_c*100/train_features_c.shape[0])\n",
    "        \n",
    "        test_preds_c = net_c(test_features_c)\n",
    "        total_val_loss_c.append(F.cross_entropy(test_preds_c, test_labels_c, loss_weights_c).item())\n",
    "        total_val_accuracy_c.append(get_correct_predictions(test_preds_c, test_labels_c)*100/test_features_c.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 60.64%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graphs and evaluation\n",
    "with torch.no_grad():\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(total_accuracy_c, label='accuracy')\n",
    "    plt.plot(total_val_accuracy_c, label = 'val_accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0, 100])\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(total_loss_c, label='loss')\n",
    "    plt.plot(total_val_loss_c, label = 'val_loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    \n",
    "    test_predictions_c = net_c(test_features_c)\n",
    "    test_correct_predictions_c = get_correct_predictions(test_predictions_c, test_labels_c)\n",
    "    print(\"Test accuracy is {:2.2f}%\" .format(test_correct_predictions_c * 100 /test_features_c.shape[0]))\n",
    "    \n",
    "    # Creating of confusion matrix\n",
    "    stacked_c = torch.stack((test_labels_c, test_predictions_c.argmax(dim=1)), dim=1)\n",
    "    confusion_matrix_c = torch.zeros((number_of_groups,number_of_groups), dtype=torch.int32)      \n",
    "    # horizontal axis - predicted, vertical - true\n",
    "    for row in stacked_c:\n",
    "        confusion_matrix_c[row[0].item()][row[1].item()] += 1     # row is target, column - predicted\n",
    "    \n",
    "    # print(confusion_matrix)\n",
    "    # print(400 - torch.count_nonzero(test_labels_tensor).item())\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(confusion_matrix_c, cmap='winter')\n",
    "    \n",
    "    ax.set_xticks(np.arange(confusion_matrix_c.shape[0]))\n",
    "    ax.set_yticks(np.arange(confusion_matrix_c.shape[1]))\n",
    "    x_labels = []\n",
    "    y_labels = []\n",
    "    for i in range(confusion_matrix_c.shape[0]):\n",
    "        x_labels.append('Predicted: '+ str(i + 1))\n",
    "        y_labels.append('Real: ' + str(i + 1))\n",
    "    ax.set_xticklabels(x_labels)\n",
    "    ax.set_yticklabels(y_labels)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    for i in range(confusion_matrix_c.shape[0]):\n",
    "        for j in range(confusion_matrix_c.shape[1]):\n",
    "            text = ax.text(j, i, str(round(confusion_matrix_c[i][j].item()*100/test_features_c.shape[0],2)) + '%', ha=\"center\", va=\"center\", size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 384])\n",
      "tensor([14.2337, 12.2296,  9.7574,  9.2377,  8.5597,  8.2438,  7.5421,  7.4534,\n",
      "         7.0556,  6.7096,  6.4875,  6.2839,  6.1072,  5.8893,  5.7214,  5.4046,\n",
      "         5.2313,  4.7018,  3.9548,  3.6412,  3.3155,  2.5899,  2.5428,  2.3384,\n",
      "         2.0638,  1.9265,  1.8057,  1.6191,  1.5641,  1.4563,  1.3649,  1.2898,\n",
      "         1.2835,  1.1827,  1.0879,  0.9628,  0.8884,  0.8610,  0.8271,  0.7966,\n",
      "         0.7394,  0.7210,  0.7143,  0.6389,  0.6141,  0.6102,  0.5710,  0.5235,\n",
      "         0.4569,  0.4334,  0.4110,  0.3898,  0.3818,  0.3746,  0.3660,  0.3490,\n",
      "         0.3425,  0.3318,  0.3187,  0.3027,  0.2965,  0.2925,  0.2803,  0.2610])\n",
      "torch.Size([16, 60])\n",
      "tensor([4.3182e+00, 3.6207e+00, 3.2013e+00, 1.4047e+00, 5.8879e-01, 3.5649e-01,\n",
      "        2.5926e-01, 2.1863e-01, 1.7469e-01, 1.3808e-01, 9.4151e-02, 8.4252e-02,\n",
      "        6.9303e-02, 1.7511e-02, 2.5389e-03, 2.2093e-04])\n",
      "Parameter containing:\n",
      "tensor([[[[ 3.1773e-01,  1.9855e-01],\n",
      "          [ 2.3873e-01,  1.7159e-02],\n",
      "          [ 1.3195e-01,  1.8063e-01],\n",
      "          [ 1.9259e-01,  1.4341e-01]],\n",
      "\n",
      "         [[ 2.2169e-01,  1.0922e-01],\n",
      "          [ 2.9643e-01,  5.3929e-01],\n",
      "          [ 2.7882e-01,  2.7181e-01],\n",
      "          [ 5.7416e-01,  5.1959e-01]],\n",
      "\n",
      "         [[ 2.7428e-02, -3.0556e-01],\n",
      "          [-1.1547e-01, -4.0321e-01],\n",
      "          [-2.2659e-01, -1.9921e-01],\n",
      "          [-9.9978e-02, -2.0342e-01]],\n",
      "\n",
      "         [[-2.2094e-01, -8.0559e-01],\n",
      "          [-3.3779e-02, -1.2320e-02],\n",
      "          [-6.6961e-02,  2.9677e-02],\n",
      "          [-1.9719e-02, -1.8547e-01]],\n",
      "\n",
      "         [[-1.8676e-02, -8.9586e-02],\n",
      "          [ 2.4894e-02,  9.4182e-02],\n",
      "          [-1.0961e-01,  1.3886e-01],\n",
      "          [ 1.7153e-02, -2.3061e-01]],\n",
      "\n",
      "         [[-3.0559e-01, -2.0871e-01],\n",
      "          [ 1.7525e-01,  2.0859e-02],\n",
      "          [-2.5031e-01, -1.6949e-01],\n",
      "          [-2.8530e-01, -5.0555e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 8.4513e-02,  1.6030e-01],\n",
      "          [ 2.9124e-01,  3.8213e-01],\n",
      "          [-6.7323e-03,  1.8041e-01],\n",
      "          [ 1.8415e-01,  5.0929e-01]],\n",
      "\n",
      "         [[-1.9231e-01,  4.1079e-01],\n",
      "          [-1.8975e-01,  2.7179e-01],\n",
      "          [ 1.0978e-01,  3.1030e-01],\n",
      "          [-5.7413e-02,  2.7237e-01]],\n",
      "\n",
      "         [[-1.0095e-02, -9.4298e-02],\n",
      "          [-1.1031e-01, -7.4620e-02],\n",
      "          [ 2.6447e-01, -1.2288e-01],\n",
      "          [-6.7625e-01, -1.6141e-01]],\n",
      "\n",
      "         [[-6.8001e-02, -1.9207e-01],\n",
      "          [-1.8764e-01, -2.8301e-01],\n",
      "          [-2.1790e-01,  1.8291e-01],\n",
      "          [ 1.9580e-02,  4.3278e-02]],\n",
      "\n",
      "         [[ 4.3747e-02, -3.8911e-01],\n",
      "          [-2.8164e-02, -2.2655e-01],\n",
      "          [-7.9412e-02,  5.3995e-02],\n",
      "          [ 1.0269e-01,  2.4184e-01]],\n",
      "\n",
      "         [[-1.5843e-01,  1.1994e-01],\n",
      "          [ 3.3833e-02,  5.2729e-02],\n",
      "          [ 1.0335e-01,  1.2359e-01],\n",
      "          [ 1.6138e-02,  2.4298e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 8.6147e-02,  3.7201e-01],\n",
      "          [ 2.2625e-01,  1.1966e-01],\n",
      "          [-6.9599e-02,  1.4135e-02],\n",
      "          [-1.0179e-01,  1.6505e-01]],\n",
      "\n",
      "         [[ 1.2398e-01,  2.5339e-01],\n",
      "          [ 1.1404e-02,  1.7809e-01],\n",
      "          [ 4.1329e-03,  2.3998e-01],\n",
      "          [-6.0168e-02,  2.1304e-02]],\n",
      "\n",
      "         [[ 4.2172e-02, -1.1835e-01],\n",
      "          [ 2.1365e-01,  1.3644e-02],\n",
      "          [ 9.8655e-02,  3.9177e-01],\n",
      "          [ 1.3113e-01,  1.0266e-01]],\n",
      "\n",
      "         [[-2.4365e-02,  2.4084e-01],\n",
      "          [ 2.4970e-02,  1.9675e-01],\n",
      "          [ 5.2083e-02, -1.2543e-01],\n",
      "          [ 4.9324e-02,  1.2831e-01]],\n",
      "\n",
      "         [[ 1.2711e-01,  1.7171e-02],\n",
      "          [ 2.5012e-02,  3.5935e-02],\n",
      "          [-1.1066e-01,  6.7701e-02],\n",
      "          [-5.2323e-02,  6.8197e-03]],\n",
      "\n",
      "         [[-4.3791e-02, -7.6320e-02],\n",
      "          [ 1.2022e-01, -2.3817e-01],\n",
      "          [-7.2447e-02,  1.9828e-02],\n",
      "          [-9.0968e-02, -2.9488e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.3690e-01, -1.1813e-01],\n",
      "          [-3.8983e-02,  5.8926e-02],\n",
      "          [ 1.1634e-01,  3.8759e-02],\n",
      "          [-1.0853e-01, -8.8695e-02]],\n",
      "\n",
      "         [[-8.9474e-02, -1.4483e-01],\n",
      "          [ 2.4723e-01,  3.7380e-01],\n",
      "          [-2.2726e-01, -2.6819e-01],\n",
      "          [-3.5240e-01,  3.6721e-01]],\n",
      "\n",
      "         [[ 3.5046e-02, -4.4520e-02],\n",
      "          [-1.2193e-02, -2.1395e-01],\n",
      "          [-3.5564e-01, -9.7810e-02],\n",
      "          [ 1.3218e-01,  2.3799e-02]],\n",
      "\n",
      "         [[-6.2635e-01, -9.2038e-02],\n",
      "          [ 2.3083e-01, -3.7684e-01],\n",
      "          [ 1.8721e-01, -4.2776e-01],\n",
      "          [ 4.4531e-01, -2.9583e-01]],\n",
      "\n",
      "         [[ 6.2186e-02, -2.2402e-01],\n",
      "          [ 4.6804e-02, -3.8498e-01],\n",
      "          [-9.6802e-02,  1.9491e-01],\n",
      "          [ 5.9482e-01,  2.2130e-01]],\n",
      "\n",
      "         [[ 1.9035e-01,  1.8420e-01],\n",
      "          [ 1.9112e-02,  7.4865e-01],\n",
      "          [ 2.0374e-01,  1.2952e-01],\n",
      "          [ 4.7612e-01,  3.2974e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.4310e-04, -1.4814e-01],\n",
      "          [ 1.5659e-02,  6.6619e-02],\n",
      "          [ 1.1658e-01,  2.1885e-01],\n",
      "          [-1.1641e-03,  2.8946e-01]],\n",
      "\n",
      "         [[ 3.0610e-01,  2.7953e-01],\n",
      "          [ 1.5431e-01,  6.9074e-03],\n",
      "          [ 3.8870e-02,  2.8215e-01],\n",
      "          [ 2.5374e-01,  2.2359e-02]],\n",
      "\n",
      "         [[-5.5608e-02, -2.5186e-01],\n",
      "          [ 3.2131e-04, -5.4087e-02],\n",
      "          [-6.8768e-02,  1.8312e-01],\n",
      "          [-1.3724e-01,  1.0153e-01]],\n",
      "\n",
      "         [[-2.8434e-02, -2.4555e-01],\n",
      "          [ 2.2375e-01,  1.4838e-01],\n",
      "          [-1.0321e-01,  8.3138e-02],\n",
      "          [-6.8823e-02,  5.8023e-02]],\n",
      "\n",
      "         [[-2.1819e-01, -2.6308e-02],\n",
      "          [ 1.4030e-01,  5.6598e-02],\n",
      "          [-1.3272e-02, -3.9651e-01],\n",
      "          [-8.4346e-02,  5.1897e-02]],\n",
      "\n",
      "         [[-3.6870e-02,  4.6713e-02],\n",
      "          [ 4.0177e-02,  2.4151e-01],\n",
      "          [-8.2751e-02,  4.5934e-02],\n",
      "          [ 1.0828e-02, -2.1118e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.8302e-01,  1.3185e-01],\n",
      "          [ 3.7948e-01,  4.8363e-01],\n",
      "          [-1.2022e-01,  1.5038e-01],\n",
      "          [ 5.6140e-02,  1.9319e-01]],\n",
      "\n",
      "         [[ 1.7601e-01,  1.5117e-01],\n",
      "          [ 4.2318e-01,  1.9221e-01],\n",
      "          [ 1.9762e-01,  3.5871e-01],\n",
      "          [ 1.8853e-01, -1.2751e-01]],\n",
      "\n",
      "         [[-1.9183e-02,  6.0269e-02],\n",
      "          [ 1.9599e-02, -5.6872e-02],\n",
      "          [-5.2119e-01,  7.1187e-02],\n",
      "          [ 1.7748e-01, -3.2413e-01]],\n",
      "\n",
      "         [[ 1.1305e-01,  3.2446e-01],\n",
      "          [ 2.3281e-01, -1.1864e-01],\n",
      "          [ 1.1229e-01, -1.3950e-01],\n",
      "          [-5.2856e-02, -4.5869e-01]],\n",
      "\n",
      "         [[ 1.4956e-02, -1.6127e-01],\n",
      "          [-3.3625e-02, -3.6738e-02],\n",
      "          [-1.4420e-01, -4.6930e-01],\n",
      "          [-2.8691e-01,  1.0018e-01]],\n",
      "\n",
      "         [[-1.1776e-01,  6.1042e-02],\n",
      "          [-3.1808e-01,  1.5090e-01],\n",
      "          [-5.2078e-02, -2.5806e-01],\n",
      "          [-1.5176e-01, -2.3949e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0132e-01,  5.8945e-02],\n",
      "          [-1.0087e-02,  1.7702e-01],\n",
      "          [ 1.1288e-01,  6.3728e-02],\n",
      "          [-3.9800e-02,  3.2023e-01]],\n",
      "\n",
      "         [[ 7.4228e-02, -1.5297e-01],\n",
      "          [ 9.4582e-02,  1.6909e-01],\n",
      "          [ 1.3488e-01,  1.8081e-01],\n",
      "          [ 3.8463e-02,  9.0170e-02]],\n",
      "\n",
      "         [[ 2.0571e-01, -2.2761e-01],\n",
      "          [ 4.8053e-02, -1.8873e-01],\n",
      "          [ 8.5109e-03,  1.5176e-01],\n",
      "          [-5.8978e-02, -7.1043e-02]],\n",
      "\n",
      "         [[-8.8915e-02, -5.5364e-02],\n",
      "          [-4.0632e-02,  2.2204e-01],\n",
      "          [-5.1119e-02,  3.9560e-02],\n",
      "          [-4.0906e-01,  5.1440e-02]],\n",
      "\n",
      "         [[ 3.5941e-01,  5.5147e-02],\n",
      "          [ 1.6177e-02,  2.0435e-01],\n",
      "          [ 9.4856e-02, -3.3332e-01],\n",
      "          [ 3.1723e-01, -1.3328e-01]],\n",
      "\n",
      "         [[ 2.7066e-01,  1.0434e-02],\n",
      "          [ 7.2778e-03, -3.3788e-01],\n",
      "          [ 1.5606e-01,  2.8600e-02],\n",
      "          [ 9.6799e-02,  2.1680e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 7.1849e-03, -2.4877e-02],\n",
      "          [ 2.3339e-01,  1.8564e-01],\n",
      "          [ 7.2743e-02, -3.1968e-04],\n",
      "          [ 1.2302e-01, -2.4015e-01]],\n",
      "\n",
      "         [[ 9.7805e-02, -1.5037e-02],\n",
      "          [-1.7127e-01,  1.8176e-02],\n",
      "          [ 6.0272e-02,  1.0608e-01],\n",
      "          [-1.5750e-03,  1.1150e-01]],\n",
      "\n",
      "         [[-4.5154e-02, -3.7922e-01],\n",
      "          [-3.5047e-01, -3.3015e-01],\n",
      "          [ 2.6975e-01, -5.8407e-01],\n",
      "          [-1.3276e-01, -4.0202e-01]],\n",
      "\n",
      "         [[-3.6831e-02, -3.6689e-01],\n",
      "          [-5.1211e-01,  1.1396e-01],\n",
      "          [-3.4198e-01, -4.9362e-01],\n",
      "          [-3.2562e-01,  1.7599e-01]],\n",
      "\n",
      "         [[-6.1019e-02,  1.1715e-01],\n",
      "          [-4.7658e-01, -4.5733e-01],\n",
      "          [-1.8836e-01, -2.2719e-01],\n",
      "          [-2.6982e-01,  6.2053e-02]],\n",
      "\n",
      "         [[-4.5884e-02, -4.2661e-01],\n",
      "          [-3.0270e-01, -7.9507e-02],\n",
      "          [ 1.2714e-01,  7.1542e-03],\n",
      "          [ 2.5157e-01, -2.3709e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2736e-01, -3.0824e-02],\n",
      "          [ 2.0886e-01,  3.8777e-02],\n",
      "          [ 2.0000e-01,  6.8988e-02],\n",
      "          [ 2.7547e-02,  7.6024e-02]],\n",
      "\n",
      "         [[ 2.2572e-02,  3.1172e-01],\n",
      "          [ 1.0303e-01,  1.1743e-01],\n",
      "          [ 2.6926e-02,  3.0703e-02],\n",
      "          [ 8.0936e-02,  2.0887e-01]],\n",
      "\n",
      "         [[ 2.6475e-02, -6.2281e-02],\n",
      "          [ 7.4505e-02,  5.5732e-02],\n",
      "          [-7.2354e-03,  3.7644e-02],\n",
      "          [-3.9843e-02,  3.1152e-02]],\n",
      "\n",
      "         [[ 8.4877e-02,  7.0833e-02],\n",
      "          [-2.0755e-02,  4.3616e-02],\n",
      "          [ 7.0323e-02,  2.2699e-02],\n",
      "          [ 2.7434e-01,  6.7595e-02]],\n",
      "\n",
      "         [[-1.0904e-01,  2.7033e-01],\n",
      "          [ 4.5258e-02,  1.2114e-01],\n",
      "          [ 2.0974e-01,  2.8142e-02],\n",
      "          [-5.6624e-02, -4.2132e-02]],\n",
      "\n",
      "         [[ 9.5505e-02,  2.0790e-02],\n",
      "          [-2.9993e-02, -1.6347e-02],\n",
      "          [ 2.7776e-01,  1.1868e-01],\n",
      "          [ 2.7224e-01,  1.2771e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.2433e-01, -4.0190e-02],\n",
      "          [-8.4497e-02, -7.6279e-02],\n",
      "          [ 6.5918e-03,  9.2902e-02],\n",
      "          [-6.0242e-02,  3.4442e-02]],\n",
      "\n",
      "         [[-7.0976e-02,  2.2187e-03],\n",
      "          [ 9.5548e-02,  1.0203e-01],\n",
      "          [ 2.5897e-02,  2.7914e-02],\n",
      "          [-7.8848e-02, -6.5627e-02]],\n",
      "\n",
      "         [[-7.7184e-02, -5.7182e-02],\n",
      "          [-1.5697e-02,  1.2348e-01],\n",
      "          [ 4.3966e-02,  4.8270e-02],\n",
      "          [ 7.0619e-02, -1.5459e-01]],\n",
      "\n",
      "         [[ 2.5710e-02, -1.2757e-01],\n",
      "          [-8.6916e-02, -6.7483e-03],\n",
      "          [ 1.4628e-01, -1.1503e-01],\n",
      "          [ 7.4370e-02, -1.4374e-01]],\n",
      "\n",
      "         [[-1.9159e-01, -2.3385e-01],\n",
      "          [-1.2697e-01, -2.0509e-02],\n",
      "          [ 7.7681e-02, -1.9022e-02],\n",
      "          [-4.6742e-02, -1.4943e-01]],\n",
      "\n",
      "         [[-2.2489e-01,  4.6557e-02],\n",
      "          [-4.6236e-04,  3.3488e-02],\n",
      "          [ 6.3886e-02, -2.4300e-02],\n",
      "          [-2.1904e-01, -2.0499e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 7.1311e-02, -1.8768e-01],\n",
      "          [ 1.7578e-01, -1.1247e-01],\n",
      "          [-3.3860e-01, -3.8367e-01],\n",
      "          [ 7.4699e-02, -7.4141e-02]],\n",
      "\n",
      "         [[-2.5531e-02, -4.5080e-02],\n",
      "          [-2.3759e-01, -9.7723e-02],\n",
      "          [-2.3799e-01, -1.9327e-01],\n",
      "          [-8.1833e-02, -2.2944e-01]],\n",
      "\n",
      "         [[-4.0192e-03,  6.7992e-02],\n",
      "          [ 4.4237e-01, -6.9890e-01],\n",
      "          [-5.1570e-02,  4.4954e-02],\n",
      "          [ 1.7791e-01,  3.9188e-01]],\n",
      "\n",
      "         [[-1.6646e-01,  2.0358e-01],\n",
      "          [-2.3202e-01,  5.9050e-01],\n",
      "          [-5.9169e-01, -9.4644e-01],\n",
      "          [-1.1766e-01, -3.0156e-01]],\n",
      "\n",
      "         [[ 2.6277e-01,  2.4926e-01],\n",
      "          [-5.5715e-01, -3.8176e-01],\n",
      "          [-5.4098e-01,  6.7944e-01],\n",
      "          [ 2.1309e-01,  1.8947e-01]],\n",
      "\n",
      "         [[ 3.5433e-01, -2.1246e-01],\n",
      "          [ 2.0107e-01,  8.0205e-02],\n",
      "          [ 6.6414e-02,  4.5309e-01],\n",
      "          [-6.5578e-01, -1.4494e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0022e-02,  4.5568e-01],\n",
      "          [ 2.2017e-01, -3.1777e-02],\n",
      "          [ 2.4201e-01,  2.1743e-01],\n",
      "          [ 2.3404e-01, -1.4490e-01]],\n",
      "\n",
      "         [[-6.4417e-02,  1.5201e-01],\n",
      "          [-1.0440e-01,  9.8056e-02],\n",
      "          [ 1.8145e-01, -2.8181e-02],\n",
      "          [ 1.6825e-01,  1.6767e-01]],\n",
      "\n",
      "         [[ 5.4012e-02,  8.0197e-02],\n",
      "          [ 2.0792e-02, -3.0692e-02],\n",
      "          [ 2.6240e-02, -1.0611e-01],\n",
      "          [-2.2511e-01,  3.6480e-01]],\n",
      "\n",
      "         [[ 4.4708e-02,  9.2502e-02],\n",
      "          [-2.0957e-02,  6.8280e-02],\n",
      "          [-2.2848e-01, -1.0866e-01],\n",
      "          [-1.5849e-01,  7.3993e-02]],\n",
      "\n",
      "         [[-7.8952e-02, -1.8385e-01],\n",
      "          [-2.3099e-01,  4.2586e-02],\n",
      "          [ 1.9660e-01,  2.5376e-02],\n",
      "          [-5.1368e-02, -1.4759e-02]],\n",
      "\n",
      "         [[ 3.9193e-01,  1.3576e-01],\n",
      "          [ 2.3017e-02,  1.0489e-01],\n",
      "          [ 1.1684e-01,  1.0460e-01],\n",
      "          [-1.7516e-01,  1.8789e-01]]]], requires_grad=True)\n",
      "torch.Size([4, 16])\n"
     ]
    }
   ],
   "source": [
    "# p1 = net_c.lin1.weight.clone().detach()\n",
    "p1 = net.lin1.weight.clone().detach()\n",
    "print(p1.shape)\n",
    "u, s1, v = torch.svd(p1)\n",
    "print(s1)\n",
    "p2 = net_c.lin2.weight.clone().detach()\n",
    "print(p2.shape)\n",
    "u, s2, v = torch.svd(p2)\n",
    "print(s2)\n",
    "\n",
    "print(net_c.conv1.weight)\n",
    "print(net_c.out.weight.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
